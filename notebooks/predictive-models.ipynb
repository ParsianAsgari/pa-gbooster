{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this segment, two models are generated:\n",
    "\n",
    "1. A model using XGBoost using one-hold-out cross-validation\n",
    "2. A model using my Regressor_GradientBoost using one-hold-out cross-validation\n",
    "\n",
    "\n",
    "References: \n",
    "1. https://www.kaggle.com/omarito/gridsearchcv-xgbregressor-0-556-lb\n",
    "2. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "3. https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
    "4. https://aiinpractice.com/xgboost-hyperparameter-tuning-with-bayesian-optimization/\n",
    "5. https://github.com/fmfn/BayesianOptimization\n",
    "6. https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "1. [Data Import & Pre-Processing](#LoadingData)\n",
    "2. [XGBoost Model](#XGBoostModel)\n",
    "3. [DIY Gradient Boosting](#DIYGradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import xgboost as xgb\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Parsian's modules\n",
    "from src import FilePaths, Data_Properties, Make_DataSet, Regressor_GradientBoost, RegressGB_Parameters, RegressXGB_Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import & Processing  <a class=\"anchor\" id=\"LoadingData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling Summary:\n",
    "\n",
    "Here are the results of the profiling done under the 'exploratory-data-analysis' JupyterNotebook:\n",
    "\n",
    "* In total there are 66 columns and 636,984 rows.\n",
    "* There are missing values in six columns:\n",
    "\t\n",
    "| Columns    | Zero Values\t| Missing Values  | % of Total Values\t| Total Zero Missing Values | % Total Zero Missing Values| Data Type\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| feature_10    | \t0    | \t69566    | \t10.9    | \t69566    | \t10.9    | \tfloat64    | \n",
    "| feature_62    | \t0    | \t69566    | \t10.9    | \t69566    | \t10.9    | \tfloat64    | \n",
    "| feature_36    | \t0    | \t37907    | \t6.0    | \t37907    | \t6.0    | \tfloat64    | \n",
    "| feature_23    | \t0    | \t34567    | \t5.4    | \t34567    | \t5.4    | \tfloat64    | \n",
    "| feature_49    | \t0    | \t34567    | \t5.4    | \t34567    | \t5.4    | \tfloat64    | \n",
    "| feature_50    | \t0    | \t6743    | \t1.1    | \t6743    | \t1.1    | \tfloat64    | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by placiong the raw .csv files into the ```/data/raw``` directory. The following block will:\n",
    "\n",
    "1. Load the CSV files\n",
    "2. Stiches the CSV files\n",
    "3. Impute the missing values (default: median of the column)\n",
    "4. Create training set: X_train, y_train and testing set: X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The following .csv files will get stitched: \n",
      "['1_record_diast.csv', '2_record_diast.csv', '3_record_diast.csv', '4_record_diast.csv', '5_record_diast.csv', '6_record_diast.csv', '7_record_diast.csv', '8_record_diast.csv', '9_record_diast.csv', '10_record_diast.csv', '11_record_diast.csv', '12_record_diast.csv', '13_record_diast.csv', '14_record_diast.csv', '15_record_diast.csv']\n",
      "Stitching is done!\n",
      "Fill null values with : median\n",
      "The target is:  target\n",
      "The features are:  ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65']\n",
      "Generated X_train, X_test, y_train, y_test with test_size:  0.2\n"
     ]
    }
   ],
   "source": [
    "data_prep = Make_DataSet()\n",
    "X_train, X_test, y_train, y_test = data_prep.load_split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to handle the missing data differently for the XGBoost and my own DIY Gradient Boost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model:  <a class=\"anchor\" id=\"XGBoostModel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement XGRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgb_regressor(X, y, colsample_bytree=0.3, learning_rate = 0.1, \n",
    "                      max_depth = 5, alpha = 10, n_estimators = 70, nthread=-1):\n",
    "    '''\n",
    "    Fits Scikit-Learn's XGBooster Regressor to the data. Returns model for One Hold Out validation.\n",
    "    '''\n",
    "    try:\n",
    "\n",
    "        cv_results = pd.DataFrame()\n",
    "        xg_reg = xgb.XGBRegressor(colsample_bytree = colsample_bytree, learning_rate = learning_rate,\n",
    "                                  max_depth = max_depth, alpha = alpha, n_estimators = n_estimators, nthread=-1)\n",
    "        model = xg_reg.fit(X, y)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def predict_rmse_rsqured(model, Xtest, ytest):\n",
    "    '''\n",
    "    Use the input model to predict y for a given xtest, in additon it calculates RMSE\n",
    "    between ytest and predictions.\n",
    "    '''\n",
    "    try:\n",
    "        prediction = model.predict(Xtest)\n",
    "        rmse = np.sqrt(metrics.mean_squared_error(ytest, prediction))\n",
    "        r_squared = metrics.r2_score(ytest, prediction)\n",
    "        print(\"RMSE: %f\" % (rmse))\n",
    "        print(\"R Squared: %f\" % (r_squared))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return prediction, rmse, r_squared\n",
    "\n",
    "\n",
    "\n",
    "def cv_xgboost_regressor(data_matrix, params, nfold=3, num_boost_round=70, \n",
    "                         early_stopping_rounds=10, metrics=\"rmse\", seed=123):\n",
    "    cv_results = xgb.cv(dtrain=data_matrix, params=params, nfold=nfold, \n",
    "                        num_boost_round=num_boost_round, early_stopping_rounds=early_stopping_rounds, \n",
    "                        metrics=metrics, as_pandas=True, seed=seed)\n",
    "    \n",
    "    print('Top 5 Cross Validation RMSEs: ', cv_results.head())\n",
    "    print('Last Cross Validation RMSE between Validation and Actual: ', (cv_results['test-rmse-mean']).tail(1))\n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a XGBoost Regressor:\n",
    "\n",
    "#### One Hold Valdiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "# Initial Paramters:\n",
    "\n",
    "colsample_bytree=0.3\n",
    "learning_rate = 0.1 \n",
    "max_depth = 5\n",
    "alpha = 10\n",
    "n_estimators = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:14:53] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "xgb_reg_oho = fit_xgb_regressor(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.430003\n",
      "R Squared: 0.892487\n"
     ]
    }
   ],
   "source": [
    "predictions, rmse, r_squared = predict_rmse_rsqured(xgb_reg_oho, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our data profiling, it was found that the range of our 'target' column is:\n",
    "\n",
    "- min(target) = -38.789069\n",
    "- max(target) = 41.215521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter space will be optimized using a Bayesian Optimization technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asgar\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n",
    "test_dmatrix = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_evaluate(max_depth, gamma, colsample_bytree):\n",
    "    params = {'eval_metric': 'rmse',\n",
    "              'max_depth': int(max_depth),\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.1,\n",
    "              'gamma': gamma,\n",
    "              'colsample_bytree': colsample_bytree}\n",
    "    # Used around 1000 boosting rounds in the full model\n",
    "    cv_result = xgb.cv(params, train_dmatrix, num_boost_round=100, nfold=3)    \n",
    "    \n",
    "    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n",
    "    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo_training_2 = BayesianOptimization(xgb_evaluate, {'max_depth': (3,7), \n",
    "                                                      'gamma': (0, 1),\n",
    "                                                      'colsample_bytree': (0.3, 0.9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-5.063   \u001b[0m | \u001b[0m 0.8975  \u001b[0m | \u001b[0m 0.8271  \u001b[0m | \u001b[0m 3.427   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-3.973   \u001b[0m | \u001b[95m 0.824   \u001b[0m | \u001b[95m 0.0841  \u001b[0m | \u001b[95m 5.071   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-4.015   \u001b[0m | \u001b[0m 0.388   \u001b[0m | \u001b[0m 0.2421  \u001b[0m | \u001b[0m 5.16    \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-3.135   \u001b[0m | \u001b[95m 0.9     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 7.0     \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-3.144   \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 7.0     \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-3.161   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 0.4767  \u001b[0m | \u001b[0m 7.0     \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-3.132   \u001b[0m | \u001b[95m 0.9     \u001b[0m | \u001b[95m 0.4669  \u001b[0m | \u001b[95m 7.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-3.156   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 7.0     \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "xgb_bo_training_2.maximize(init_points=3, n_iter=5, acq='ei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9, 'gamma': 0.4668672005782674, 'max_depth': 7}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_params = xgb_bo_training_2.max['params']\n",
    "optimized_params['max_depth'] = int(optimized_params['max_depth'])\n",
    "optimized_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimized_params = xgb.train(optimized_params, train_dmatrix, num_boost_round=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction on Y Test Data RMSE: 1.8731620555189703\n",
      "Model Prediction on Y Train Data RMSE: 1.6518342619805442\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb = model_optimized_params.predict(test_dmatrix)\n",
    "y_train_pred_xgb = model_optimized_params.predict(train_dmatrix)\n",
    "\n",
    "print('Model Prediction on Y Test Data RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_xgb)))\n",
    "print('Model Prediction on Y Train Data RMSE:', np.sqrt(metrics.mean_squared_error(y_train, y_train_pred_xgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved under:  <_io.BufferedWriter name='C:\\\\Users\\\\asgar\\\\Documents\\\\InterviewAssignments\\\\huami-interview\\\\models\\\\Jupyter-XGBoost_Regressor_Model_Pickled.pkl'>\n"
     ]
    }
   ],
   "source": [
    "pkl_fname = \"Jupyter-XGBoost_Regressor_Model_Pickled.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(FilePaths.path_models, pkl_fname), 'wb') as file:\n",
    "    pickle.dump(model_optimized_params, file)\n",
    "\n",
    "print('Model Saved under: ', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY Gradient Boosting Regressor  <a class=\"anchor\" id=\"DIYGradientBoostingRegressor\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are hyper parameters set to train the model: \n",
      "max_depth:  5\n",
      "ntrees:  250\n",
      "learning_rate:  0.1\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "RegressGB_Parameters.ntrees = 250\n",
    "RegressGB_Parameters.max_depth = 5\n",
    "max_depth = RegressGB_Parameters.max_depth\n",
    "ntrees = RegressGB_Parameters.ntrees\n",
    "learning_rate = RegressGB_Parameters.learning_rate\n",
    "\n",
    "print('These are hyper parameters set to train the model: ')\n",
    "print('max_depth: ', max_depth)\n",
    "print('ntrees: ', ntrees)\n",
    "print('learning_rate: ', learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Regressor_GradientBoost model:\n",
    "\n",
    "Fit the model using the model's decision tree method (Based on Scikit-learn 's DecisionTreeRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regressor_gradientboost(X_train, y_train):\n",
    "        '''\n",
    "        Trains a regressor base on Parsian's pa_ml_utils.Regressor_GradientBoost\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : A Pandas DataFrame for features\n",
    "            \n",
    "        y_train : A Pandas DataFrame for target\n",
    "                \n",
    "            \n",
    "        Returns:\n",
    "            A regressor model base on pa_ml_utils.Regressor_GradientBoost\n",
    "            \n",
    "        '''     \n",
    "        try:            \n",
    "            \n",
    "            print('Initiating the training process using \\'pa-gb\\' model')\n",
    "            \n",
    "            regressor = Regressor_GradientBoost(features_df = X_train, \n",
    "                                                target = y_train,\n",
    "                                                max_depth = RegressGB_Parameters.max_depth, \n",
    "                                                ntrees = RegressGB_Parameters.ntrees, \n",
    "                                                learning_rate = RegressGB_Parameters.learning_rate)\n",
    "        \n",
    "            f0, models, training_rmse = regressor.boost_gradient(X_train, y_train)\n",
    "            print('Training completed')\n",
    "    \n",
    "        except Exception as e:\n",
    "            print('modeller.py Model_Trainer.train_regressor_gradientboost(): ',e)\n",
    "            \n",
    "        \n",
    "        return f0, models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating the training process using 'pa-gb' model\n",
      "RMSE at first prediction:  13.515627188810816\n",
      "RMSE for tree #0 is: 12.485939437579958\n",
      "RMSE for tree #1 is: 11.579720790341586\n",
      "RMSE for tree #2 is: 10.78342861420375\n",
      "RMSE for tree #3 is: 10.085339499878417\n",
      "RMSE for tree #4 is: 9.475547415495809\n",
      "RMSE for tree #5 is: 8.941390445559952\n",
      "RMSE for tree #6 is: 8.474762111073325\n",
      "RMSE for tree #7 is: 8.071758154138564\n",
      "RMSE for tree #8 is: 7.711799416401938\n",
      "RMSE for tree #9 is: 7.394097540510946\n",
      "RMSE for tree #10 is: 7.124403786713729\n",
      "RMSE for tree #11 is: 6.896506465826787\n",
      "RMSE for tree #12 is: 6.688255426918239\n",
      "RMSE for tree #13 is: 6.515810619025053\n",
      "RMSE for tree #14 is: 6.358874639850079\n",
      "RMSE for tree #15 is: 6.222790412855806\n",
      "RMSE for tree #16 is: 6.093898958071987\n",
      "RMSE for tree #17 is: 5.980941872890951\n",
      "RMSE for tree #18 is: 5.876440397941171\n",
      "RMSE for tree #19 is: 5.795758705727741\n",
      "RMSE for tree #20 is: 5.718405388793373\n",
      "RMSE for tree #21 is: 5.64608381828004\n",
      "RMSE for tree #22 is: 5.593104303242884\n",
      "RMSE for tree #23 is: 5.531489607588546\n",
      "RMSE for tree #24 is: 5.480806827520391\n",
      "RMSE for tree #25 is: 5.431299463417782\n",
      "RMSE for tree #26 is: 5.380732536413963\n",
      "RMSE for tree #27 is: 5.3201480602187745\n",
      "RMSE for tree #28 is: 5.270208973991952\n",
      "RMSE for tree #29 is: 5.234698866435188\n",
      "RMSE for tree #30 is: 5.201727812498848\n",
      "RMSE for tree #31 is: 5.164927658819103\n",
      "RMSE for tree #32 is: 5.1192091063574425\n",
      "RMSE for tree #33 is: 5.086026845363543\n",
      "RMSE for tree #34 is: 5.047799093923927\n",
      "RMSE for tree #35 is: 5.0239174963586235\n",
      "RMSE for tree #36 is: 4.987672604099027\n",
      "RMSE for tree #37 is: 4.939574785365044\n",
      "RMSE for tree #38 is: 4.920269848684776\n",
      "RMSE for tree #39 is: 4.898804666037072\n",
      "RMSE for tree #40 is: 4.870362709965085\n",
      "RMSE for tree #41 is: 4.841304602117682\n",
      "RMSE for tree #42 is: 4.8033837823489955\n",
      "RMSE for tree #43 is: 4.775543852786384\n",
      "RMSE for tree #44 is: 4.75476162495573\n",
      "RMSE for tree #45 is: 4.70847333368319\n",
      "RMSE for tree #46 is: 4.697617156614774\n",
      "RMSE for tree #47 is: 4.672083711997403\n",
      "RMSE for tree #48 is: 4.641051401059713\n",
      "RMSE for tree #49 is: 4.624711546118517\n",
      "RMSE for tree #50 is: 4.6025321126341385\n",
      "RMSE for tree #51 is: 4.592680998746846\n",
      "RMSE for tree #52 is: 4.571689615530771\n",
      "RMSE for tree #53 is: 4.559530901353922\n",
      "RMSE for tree #54 is: 4.536275483584771\n",
      "RMSE for tree #55 is: 4.511551997484645\n",
      "RMSE for tree #56 is: 4.496526669000663\n",
      "RMSE for tree #57 is: 4.4677332764267055\n",
      "RMSE for tree #58 is: 4.447301532196042\n",
      "RMSE for tree #59 is: 4.439473565672672\n",
      "RMSE for tree #60 is: 4.419491784554021\n",
      "RMSE for tree #61 is: 4.407047665864166\n",
      "RMSE for tree #62 is: 4.382338666611914\n",
      "RMSE for tree #63 is: 4.366662515855926\n",
      "RMSE for tree #64 is: 4.357836738336944\n",
      "RMSE for tree #65 is: 4.343619459314618\n",
      "RMSE for tree #66 is: 4.328915211971344\n",
      "RMSE for tree #67 is: 4.303676167095986\n",
      "RMSE for tree #68 is: 4.283627381677457\n",
      "RMSE for tree #69 is: 4.269506847094484\n",
      "RMSE for tree #70 is: 4.260117129907032\n",
      "RMSE for tree #71 is: 4.255415310353687\n",
      "RMSE for tree #72 is: 4.247731383382993\n",
      "RMSE for tree #73 is: 4.240056131672901\n",
      "RMSE for tree #74 is: 4.233117087447253\n",
      "RMSE for tree #75 is: 4.223317520009495\n",
      "RMSE for tree #76 is: 4.21388509883755\n",
      "RMSE for tree #77 is: 4.20086218614811\n",
      "RMSE for tree #78 is: 4.191209000090667\n",
      "RMSE for tree #79 is: 4.18451345499587\n",
      "RMSE for tree #80 is: 4.171664243692308\n",
      "RMSE for tree #81 is: 4.157373725298157\n",
      "RMSE for tree #82 is: 4.1486316480115\n",
      "RMSE for tree #83 is: 4.137660519909802\n",
      "RMSE for tree #84 is: 4.126918946294016\n",
      "RMSE for tree #85 is: 4.116690996609314\n",
      "RMSE for tree #86 is: 4.108997373004168\n",
      "RMSE for tree #87 is: 4.097296596678185\n",
      "RMSE for tree #88 is: 4.084812267120755\n",
      "RMSE for tree #89 is: 4.074612421986051\n",
      "RMSE for tree #90 is: 4.067362117878531\n",
      "RMSE for tree #91 is: 4.060514572423506\n",
      "RMSE for tree #92 is: 4.055307830547341\n",
      "RMSE for tree #93 is: 4.045341709651186\n",
      "RMSE for tree #94 is: 4.032549576303636\n",
      "RMSE for tree #95 is: 4.023081839563605\n",
      "RMSE for tree #96 is: 4.012426016181193\n",
      "RMSE for tree #97 is: 4.00565942177046\n",
      "RMSE for tree #98 is: 4.000083860248085\n",
      "RMSE for tree #99 is: 3.9781088096480577\n",
      "RMSE for tree #100 is: 3.9667301693685566\n",
      "RMSE for tree #101 is: 3.948495124670893\n",
      "RMSE for tree #102 is: 3.9367304478311658\n",
      "RMSE for tree #103 is: 3.92875604442072\n",
      "RMSE for tree #104 is: 3.9167755105333106\n",
      "RMSE for tree #105 is: 3.9087460211400087\n",
      "RMSE for tree #106 is: 3.9008273120399783\n",
      "RMSE for tree #107 is: 3.8927776232798204\n",
      "RMSE for tree #108 is: 3.8694967952533417\n",
      "RMSE for tree #109 is: 3.8646154698365254\n",
      "RMSE for tree #110 is: 3.857060501731401\n",
      "RMSE for tree #111 is: 3.8488734809800818\n",
      "RMSE for tree #112 is: 3.842002539918949\n",
      "RMSE for tree #113 is: 3.8368785674377888\n",
      "RMSE for tree #114 is: 3.8310512538912453\n",
      "RMSE for tree #115 is: 3.826363885567077\n",
      "RMSE for tree #116 is: 3.814753647691128\n",
      "RMSE for tree #117 is: 3.806191912498073\n",
      "RMSE for tree #118 is: 3.8027941658100373\n",
      "RMSE for tree #119 is: 3.7983312361436017\n",
      "RMSE for tree #120 is: 3.7907878590643245\n",
      "RMSE for tree #121 is: 3.7839766465219413\n",
      "RMSE for tree #122 is: 3.7739221714786875\n",
      "RMSE for tree #123 is: 3.7701253778368\n",
      "RMSE for tree #124 is: 3.7645542672976156\n",
      "RMSE for tree #125 is: 3.7574011870590764\n",
      "RMSE for tree #126 is: 3.7471830024121857\n",
      "RMSE for tree #127 is: 3.741166609376105\n",
      "RMSE for tree #128 is: 3.7362779217002675\n",
      "RMSE for tree #129 is: 3.7267037886433036\n",
      "RMSE for tree #130 is: 3.7205351897147194\n",
      "RMSE for tree #131 is: 3.716116811511385\n",
      "RMSE for tree #132 is: 3.7133967741629013\n",
      "RMSE for tree #133 is: 3.7095621478482923\n",
      "RMSE for tree #134 is: 3.7026280961805713\n",
      "RMSE for tree #135 is: 3.6985957807677883\n",
      "RMSE for tree #136 is: 3.6931995447748474\n",
      "RMSE for tree #137 is: 3.6857705979241717\n",
      "RMSE for tree #138 is: 3.6801149679397036\n",
      "RMSE for tree #139 is: 3.674320534986667\n",
      "RMSE for tree #140 is: 3.665247541155389\n",
      "RMSE for tree #141 is: 3.65316330031682\n",
      "RMSE for tree #142 is: 3.6470041163626705\n",
      "RMSE for tree #143 is: 3.641655370934831\n",
      "RMSE for tree #144 is: 3.637275943577232\n",
      "RMSE for tree #145 is: 3.631657070506641\n",
      "RMSE for tree #146 is: 3.624260651978782\n",
      "RMSE for tree #147 is: 3.620495439415639\n",
      "RMSE for tree #148 is: 3.6124598423321457\n",
      "RMSE for tree #149 is: 3.6042078634331354\n",
      "RMSE for tree #150 is: 3.6012980941651365\n",
      "RMSE for tree #151 is: 3.596679765363968\n",
      "RMSE for tree #152 is: 3.5924762635714487\n",
      "RMSE for tree #153 is: 3.5872087562165853\n",
      "RMSE for tree #154 is: 3.5780344300798643\n",
      "RMSE for tree #155 is: 3.5722875637744003\n",
      "RMSE for tree #156 is: 3.5651886379025903\n",
      "RMSE for tree #157 is: 3.557387340424754\n",
      "RMSE for tree #158 is: 3.5519891836548263\n",
      "RMSE for tree #159 is: 3.5466223455350403\n",
      "RMSE for tree #160 is: 3.539379826460999\n",
      "RMSE for tree #161 is: 3.529320808905817\n",
      "RMSE for tree #162 is: 3.5252762911479834\n",
      "RMSE for tree #163 is: 3.508138789121358\n",
      "RMSE for tree #164 is: 3.5013818778212173\n",
      "RMSE for tree #165 is: 3.4988520807662993\n",
      "RMSE for tree #166 is: 3.489406091759029\n",
      "RMSE for tree #167 is: 3.481704999903256\n",
      "RMSE for tree #168 is: 3.4752576927201937\n",
      "RMSE for tree #169 is: 3.4715732264404746\n",
      "RMSE for tree #170 is: 3.4659386848232376\n",
      "RMSE for tree #171 is: 3.455085806522475\n",
      "RMSE for tree #172 is: 3.4504581130940584\n",
      "RMSE for tree #173 is: 3.4464325730813066\n",
      "RMSE for tree #174 is: 3.4420452155477452\n",
      "RMSE for tree #175 is: 3.4372922394130607\n",
      "RMSE for tree #176 is: 3.4293652615125425\n",
      "RMSE for tree #177 is: 3.422178721205286\n",
      "RMSE for tree #178 is: 3.4182920970950237\n",
      "RMSE for tree #179 is: 3.4146298353107376\n",
      "RMSE for tree #180 is: 3.41038962094702\n",
      "RMSE for tree #181 is: 3.4058683777108385\n",
      "RMSE for tree #182 is: 3.4008972372589152\n",
      "RMSE for tree #183 is: 3.3977929872601225\n",
      "RMSE for tree #184 is: 3.3895774721049663\n",
      "RMSE for tree #185 is: 3.3818782540491443\n",
      "RMSE for tree #186 is: 3.3790786899278746\n",
      "RMSE for tree #187 is: 3.3736012180572637\n",
      "RMSE for tree #188 is: 3.3697719572536333\n",
      "RMSE for tree #189 is: 3.3662934324602496\n",
      "RMSE for tree #190 is: 3.3637892064714072\n",
      "RMSE for tree #191 is: 3.3597893256284754\n",
      "RMSE for tree #192 is: 3.3489540643515325\n",
      "RMSE for tree #193 is: 3.3433390291529\n",
      "RMSE for tree #194 is: 3.3389465302032644\n",
      "RMSE for tree #195 is: 3.331294497270814\n",
      "RMSE for tree #196 is: 3.3280377232426304\n",
      "RMSE for tree #197 is: 3.322599945570877\n",
      "RMSE for tree #198 is: 3.318564026580407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for tree #199 is: 3.311221112355766\n",
      "RMSE for tree #200 is: 3.3086945467326285\n",
      "RMSE for tree #201 is: 3.3027306095511304\n",
      "RMSE for tree #202 is: 3.2982219545722002\n",
      "RMSE for tree #203 is: 3.294260477757347\n",
      "RMSE for tree #204 is: 3.2907258912189503\n",
      "RMSE for tree #205 is: 3.2839119097399347\n",
      "RMSE for tree #206 is: 3.2779516545244074\n",
      "RMSE for tree #207 is: 3.2754487721770977\n",
      "RMSE for tree #208 is: 3.266925339431294\n",
      "RMSE for tree #209 is: 3.2608122677484643\n",
      "RMSE for tree #210 is: 3.255909798702393\n",
      "RMSE for tree #211 is: 3.251006381542861\n",
      "RMSE for tree #212 is: 3.247743926751302\n",
      "RMSE for tree #213 is: 3.2437989662872777\n",
      "RMSE for tree #214 is: 3.24112913280755\n",
      "RMSE for tree #215 is: 3.2373367043377117\n",
      "RMSE for tree #216 is: 3.232885160430778\n",
      "RMSE for tree #217 is: 3.2296678032175294\n",
      "RMSE for tree #218 is: 3.225912296537506\n",
      "RMSE for tree #219 is: 3.2212635361685433\n",
      "RMSE for tree #220 is: 3.218254940135668\n",
      "RMSE for tree #221 is: 3.216058414839148\n",
      "RMSE for tree #222 is: 3.2128627849998215\n",
      "RMSE for tree #223 is: 3.2088036261185247\n",
      "RMSE for tree #224 is: 3.2043235133664636\n",
      "RMSE for tree #225 is: 3.200852433678198\n",
      "RMSE for tree #226 is: 3.1967131143326633\n",
      "RMSE for tree #227 is: 3.193180808870949\n",
      "RMSE for tree #228 is: 3.1906843459658893\n",
      "RMSE for tree #229 is: 3.1891157243689197\n",
      "RMSE for tree #230 is: 3.18060712041955\n",
      "RMSE for tree #231 is: 3.173981913258572\n",
      "RMSE for tree #232 is: 3.171522417889802\n",
      "RMSE for tree #233 is: 3.168730416253524\n",
      "RMSE for tree #234 is: 3.167290113325293\n",
      "RMSE for tree #235 is: 3.163848163059448\n",
      "RMSE for tree #236 is: 3.1608364405917198\n",
      "RMSE for tree #237 is: 3.15740911216043\n",
      "RMSE for tree #238 is: 3.1554407328683105\n",
      "RMSE for tree #239 is: 3.1510015879400957\n",
      "RMSE for tree #240 is: 3.1475328041727684\n",
      "RMSE for tree #241 is: 3.1464469430334425\n",
      "RMSE for tree #242 is: 3.1431619653326606\n",
      "RMSE for tree #243 is: 3.135899614630139\n",
      "RMSE for tree #244 is: 3.1294892159321686\n",
      "RMSE for tree #245 is: 3.12563561872532\n",
      "RMSE for tree #246 is: 3.1230929237222256\n",
      "RMSE for tree #247 is: 3.119703864937747\n",
      "RMSE for tree #248 is: 3.115442447808916\n",
      "RMSE for tree #249 is: 3.112849183636682\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "f0, models = train_regressor_gradientboost(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using the Regressor_GradientBoost model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above model, we are going to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_regressor_gradientboost(X_test, y_test, f0, reg_gb_models):\n",
    "        '''\n",
    "        Generates a prediction base on Parsian's \n",
    "        pa_ml_utils.Regressor_GradientBoost model\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : A Pandas DataFrame for features\n",
    "            \n",
    "        y_test : A Pandas Series for target\n",
    "                \n",
    "            \n",
    "        Returns:\n",
    "            Predictions base on reg_gb_models generated from pa_ml_utils.Regressor_GradientBoost\n",
    "            \n",
    "        '''        \n",
    "        try:            \n",
    "            print('Initiating the prediction process using \\'pa-gb\\' model')\n",
    "            \n",
    "            regressor = Regressor_GradientBoost(X_test, y_test)\n",
    "            prediction = regressor.predict(X_test, f0, reg_gb_models)\n",
    "            rmse = regressor.rmse(y_test, prediction)\n",
    "            \n",
    "            print('Prediction completed')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('modeller.py Model_Predictor.predict_regressor_gradientboost(): ',e)\n",
    "            \n",
    "        print(\"The RMSE for the Regressor Gradient Boost model: \", rmse)\n",
    "        return prediction, rmse   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating the prediction process using 'pa-gb' model\n",
      "Prediction completed\n",
      "The RMSE for the Regressor Gradient Boost model:  12.738929306363474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.07704991,  1.45827648, 11.97125205, ..., 25.35474394,\n",
       "        32.84679981, 10.64980486]), 12.738929306363474)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_regressor_gradientboost(X_test, y_test, f0, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost model, with some hyper parameter optimization generated a model that predicted target with RMSE of 1.87.\n",
    "\n",
    "In comparison, my model without any hyper parameter tuning, produced a prediction with RMSE of 12.74. Definitely, cross validation and hyper parameter techniques can be used to insure we are not overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
