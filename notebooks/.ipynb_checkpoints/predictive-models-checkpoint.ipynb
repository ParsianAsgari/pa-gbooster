{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this segment, two models are generated:\n",
    "\n",
    "1. A model using XGBoost using one-hold-out cross-validation\n",
    "2. A model using my Regressor_GradientBoost using one-hold-out cross-validation\n",
    "\n",
    "\n",
    "References: \n",
    "1. https://www.kaggle.com/omarito/gridsearchcv-xgbregressor-0-556-lb\n",
    "2. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "3. https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
    "4. https://aiinpractice.com/xgboost-hyperparameter-tuning-with-bayesian-optimization/\n",
    "5. https://github.com/fmfn/BayesianOptimization\n",
    "6. https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "1. [Data Import & Pre-Processing](#LoadingData)\n",
    "2. [XGBoost Model](#XGBoostModel)\n",
    "3. [DIY Gradient Boosting](#DIYGradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import xgboost as xgb\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Parsian's modules\n",
    "from src import FilePaths, Data_Properties, Make_DataSet, Regressor_GradientBoost, RegressGB_Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import & Processing  <a class=\"anchor\" id=\"LoadingData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling Summary:\n",
    "\n",
    "Here are the results of the profiling done under the 'exploratory-data-analysis' JupyterNotebook:\n",
    "\n",
    "* In total there are 66 columns and 636,984 rows.\n",
    "* There are missing values in six columns:\n",
    "\t\n",
    "| Columns    | Zero Values\t| Missing Values  | % of Total Values\t| Total Zero Missing Values | % Total Zero Missing Values| Data Type\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| feature_10    | \t0    | \t69566    | \t10.9    | \t69566    | \t10.9    | \tfloat64    | \n",
    "| feature_62    | \t0    | \t69566    | \t10.9    | \t69566    | \t10.9    | \tfloat64    | \n",
    "| feature_36    | \t0    | \t37907    | \t6.0    | \t37907    | \t6.0    | \tfloat64    | \n",
    "| feature_23    | \t0    | \t34567    | \t5.4    | \t34567    | \t5.4    | \tfloat64    | \n",
    "| feature_49    | \t0    | \t34567    | \t5.4    | \t34567    | \t5.4    | \tfloat64    | \n",
    "| feature_50    | \t0    | \t6743    | \t1.1    | \t6743    | \t1.1    | \tfloat64    | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by placiong the raw .csv files into the ```/data/raw``` directory. The following block will:\n",
    "\n",
    "1. Load the CSV files\n",
    "2. Stiches the CSV files\n",
    "3. Impute the missing values (default: median of the column)\n",
    "4. Create training set: X_train, y_train and testing set: X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The following .csv files will get stitched: \n",
      "['1_record_diast.csv', '2_record_diast.csv']\n",
      "Stitching is done!\n",
      "Fill null values with : median\n",
      "The target is:  target\n",
      "The features are:  ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65']\n",
      "Generated X_train, X_test, y_train, y_test with test_size:  0.2\n"
     ]
    }
   ],
   "source": [
    "data_prep = Make_DataSet()\n",
    "X_train, X_test, y_train, y_test = data_prep.load_split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to handle the missing data differently for the XGBoost and my own DIY Gradient Boost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model:  <a class=\"anchor\" id=\"XGBoostModel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Matrix for Scikit-learn's Cross Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X_analysis_df_preprocessed,label=y_analysis_df_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement XGRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgb_regressor(X, y, colsample_bytree=0.3, learning_rate = 0.1, \n",
    "                      max_depth = 5, alpha = 10, n_estimators = 70, nthread=-1):\n",
    "    '''\n",
    "    Fits Scikit-Learn's XGBooster Regressor to the data. Returns model for One Hold Out validation.\n",
    "    '''\n",
    "    try:\n",
    "\n",
    "        cv_results = pd.DataFrame()\n",
    "        xg_reg = xgb.XGBRegressor(colsample_bytree = colsample_bytree, learning_rate = learning_rate,\n",
    "                                  max_depth = max_depth, alpha = alpha, n_estimators = n_estimators, nthread=-1)\n",
    "        model = xg_reg.fit(X, y)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def predict_rmse_rsqured(model, Xtest, ytest):\n",
    "    '''\n",
    "    Use the input model to predict y for a given xtest, in additon it calculates RMSE\n",
    "    between ytest and predictions.\n",
    "    '''\n",
    "    try:\n",
    "        prediction = model.predict(Xtest)\n",
    "        rmse = np.sqrt(metrics.mean_squared_error(ytest, prediction))\n",
    "        r_squared = metrics.r2_score(ytest, prediction)\n",
    "        print(\"RMSE: %f\" % (rmse))\n",
    "        print(\"R Squared: %f\" % (r_squared))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return prediction, rmse, r_squared\n",
    "\n",
    "\n",
    "\n",
    "def cv_xgboost_regressor(data_matrix, params, nfold=3, num_boost_round=70, \n",
    "                         early_stopping_rounds=10, metrics=\"rmse\", seed=123):\n",
    "    cv_results = xgb.cv(dtrain=data_matrix, params=params, nfold=nfold, \n",
    "                        num_boost_round=num_boost_round, early_stopping_rounds=early_stopping_rounds, \n",
    "                        metrics=metrics, as_pandas=True, seed=seed)\n",
    "    \n",
    "    print('Top 5 Cross Validation RMSEs: ', cv_results.head())\n",
    "    print('Last Cross Validation RMSE between Validation and Actual: ', (cv_results['test-rmse-mean']).tail(1))\n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a XGBoost Regressor:\n",
    "\n",
    "#### One Hold Valdiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asgar\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:19:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "xgb_reg_oho = fit_xgb_regressor(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.089808\n",
      "R Squared: 0.979786\n"
     ]
    }
   ],
   "source": [
    "predictions, rmse, r_squared = predict_rmse_rsqured(xgb_reg_oho, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our data profiling, it was found that the range of our 'target' column is:\n",
    "\n",
    "- min(target) = -38.789069\n",
    "- max(target) = 41.215521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter space will be optimized using a Bayesian Optimization technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asgar\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n",
    "test_dmatrix = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_evaluate(max_depth, gamma, colsample_bytree):\n",
    "    params = {'eval_metric': 'rmse',\n",
    "              'max_depth': int(max_depth),\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.1,\n",
    "              'gamma': gamma,\n",
    "              'colsample_bytree': colsample_bytree}\n",
    "    # Used around 1000 boosting rounds in the full model\n",
    "    cv_result = xgb.cv(params, train_dmatrix, num_boost_round=100, nfold=3)    \n",
    "    \n",
    "    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n",
    "    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo_training = BayesianOptimization(xgb_evaluate, {'max_depth': (3,5), \n",
    "                                                      'gamma': (0, 1),\n",
    "                                                      'colsample_bytree': (0.3, 0.5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.556   \u001b[0m | \u001b[0m 0.3677  \u001b[0m | \u001b[0m 0.2054  \u001b[0m | \u001b[0m 3.337   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-2.234   \u001b[0m | \u001b[95m 0.3053  \u001b[0m | \u001b[95m 0.3433  \u001b[0m | \u001b[95m 4.797   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-2.591   \u001b[0m | \u001b[0m 0.3072  \u001b[0m | \u001b[0m 0.2065  \u001b[0m | \u001b[0m 3.152   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-1.937   \u001b[0m | \u001b[95m 0.3     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 5.0     \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m-1.869   \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 5.0     \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-1.869   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-1.869   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-1.869   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "xgb_bo_training.maximize(init_points=3, n_iter=5, acq='ei')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo_training_2 = BayesianOptimization(xgb_evaluate, {'max_depth': (3,7), \n",
    "                                                      'gamma': (0, 1),\n",
    "                                                      'colsample_bytree': (0.3, 0.9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.467   \u001b[0m | \u001b[0m 0.8756  \u001b[0m | \u001b[0m 0.3938  \u001b[0m | \u001b[0m 3.815   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-1.861   \u001b[0m | \u001b[95m 0.7061  \u001b[0m | \u001b[95m 0.06226 \u001b[0m | \u001b[95m 5.155   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-1.611   \u001b[0m | \u001b[95m 0.5314  \u001b[0m | \u001b[95m 0.9767  \u001b[0m | \u001b[95m 6.232   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-1.431   \u001b[0m | \u001b[95m 0.3     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 7.0     \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m-1.401   \u001b[0m | \u001b[95m 0.9     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 7.0     \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m-1.4     \u001b[0m | \u001b[95m 0.9     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 7.0     \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-1.395   \u001b[0m | \u001b[95m 0.9     \u001b[0m | \u001b[95m 0.5177  \u001b[0m | \u001b[95m 7.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-1.43    \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 7.0     \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "xgb_bo_training_2.maximize(init_points=3, n_iter=5, acq='ei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9, 'gamma': 0.5176518530657455, 'max_depth': 7}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_params = xgb_bo_training_2.max['params']\n",
    "optimized_params['max_depth'] = int(optimized_params['max_depth'])\n",
    "optimized_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimized_params = xgb.train(optimized_params, train_dmatrix, num_boost_round=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction on Y Test Data RMSE: 0.7852291472417188\n",
      "Model Prediction on Y Train Data RMSE: 0.4499462852907251\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb = model_optimized_params.predict(test_dmatrix)\n",
    "y_train_pred_xgb = model_optimized_params.predict(train_dmatrix)\n",
    "\n",
    "print('Model Prediction on Y Test Data RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_xgb)))\n",
    "print('Model Prediction on Y Train Data RMSE:', np.sqrt(metrics.mean_squared_error(y_train, y_train_pred_xgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved under:  <_io.BufferedWriter name='C:\\\\Users\\\\asgar\\\\Documents\\\\InterviewAssignments\\\\huami-interview\\\\models\\\\XGBoost_Regressor_Model_Pickled.pkl'>\n"
     ]
    }
   ],
   "source": [
    "pkl_fname = \"XGBoost_Regressor_Model_Pickled.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(FilePaths.path_models, \"XGBoost_Regressor_Model_Pickled.pkl\"), 'wb') as file:\n",
    "    pickle.dump(model_optimized_params, file)\n",
    "\n",
    "print('Model Saved under: ', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY Gradient Boosting Regressor  <a class=\"anchor\" id=\"DIYGradientBoostingRegressor\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are hyper parameters set to train the model: \n",
      "max_depth:  2\n",
      "ntrees:  20\n",
      "learning_rate:  0.1\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "RegressGB_Parameters.ntrees = 20\n",
    "max_depth = RegressGB_Parameters.max_depth\n",
    "ntrees = RegressGB_Parameters.ntrees\n",
    "learning_rate = RegressGB_Parameters.learning_rate\n",
    "\n",
    "print('These are hyper parameters set to train the model: ')\n",
    "print('max_depth: ', max_depth)\n",
    "print('ntrees: ', ntrees)\n",
    "print('learning_rate: ', learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Regressor_GradientBoost model:\n",
    "\n",
    "Fit the model using the model's decision tree method (Based on Scikit-learn 's DecisionTreeRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regressor_gradientboost(X_train, y_train):\n",
    "        '''\n",
    "        Trains a regressor base on Parsian's pa_ml_utils.Regressor_GradientBoost\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : A Pandas DataFrame for features\n",
    "            \n",
    "        y_train : A Pandas DataFrame for target\n",
    "                \n",
    "            \n",
    "        Returns:\n",
    "            A regressor model base on pa_ml_utils.Regressor_GradientBoost\n",
    "            \n",
    "        '''     \n",
    "        try:            \n",
    "            \n",
    "            print('Initiating the training process using \\'pa-gb\\' model')\n",
    "            \n",
    "            regressor = Regressor_GradientBoost(features_df = X_train, \n",
    "                                                target = y_train,\n",
    "                                                max_depth = RegressGB_Parameters.max_depth, \n",
    "                                                ntrees = RegressGB_Parameters.ntrees, \n",
    "                                                learning_rate = RegressGB_Parameters.learning_rate)\n",
    "        \n",
    "            f0, models, training_rmse = regressor.boost_gradient(X_train, y_train)\n",
    "            print('Training completed')\n",
    "    \n",
    "        except Exception as e:\n",
    "            print('modeller.py Model_Trainer.train_regressor_gradientboost(): ',e)\n",
    "            \n",
    "        \n",
    "        return f0, models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating the training process using 'pa-gb' model\n",
      "RMSE at first prediction:  14.664271679060443\n",
      "RMSE for tree #0 is: 13.391159114254439\n",
      "RMSE for tree #1 is: 12.258264577646218\n",
      "RMSE for tree #2 is: 11.242761813387368\n",
      "RMSE for tree #3 is: 10.337511945266398\n",
      "RMSE for tree #4 is: 9.528834549051984\n",
      "RMSE for tree #5 is: 8.7984266207075\n",
      "RMSE for tree #6 is: 8.158532061912522\n",
      "RMSE for tree #7 is: 7.587525664343998\n",
      "RMSE for tree #8 is: 7.0756400429759445\n",
      "RMSE for tree #9 is: 6.622592722475333\n",
      "RMSE for tree #10 is: 6.22578034759417\n",
      "RMSE for tree #11 is: 5.87069055985789\n",
      "RMSE for tree #12 is: 5.563028020753392\n",
      "RMSE for tree #13 is: 5.292628346405744\n",
      "RMSE for tree #14 is: 5.053040195041686\n",
      "RMSE for tree #15 is: 4.846294597175786\n",
      "RMSE for tree #16 is: 4.664026103785394\n",
      "RMSE for tree #17 is: 4.505730496796824\n",
      "RMSE for tree #18 is: 4.36898626075419\n",
      "RMSE for tree #19 is: 4.249182999171462\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "f0, models = train_regressor_gradientboost(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using the Regressor_GradientBoost model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above model, we are going to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_regressor_gradientboost(X_test, y_test, f0, reg_gb_models):\n",
    "        '''\n",
    "        Generates a prediction base on Parsian's \n",
    "        pa_ml_utils.Regressor_GradientBoost model\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : A Pandas DataFrame for features\n",
    "            \n",
    "        y_test : A Pandas Series for target\n",
    "                \n",
    "            \n",
    "        Returns:\n",
    "            Predictions base on reg_gb_models generated from pa_ml_utils.Regressor_GradientBoost\n",
    "            \n",
    "        '''        \n",
    "        try:            \n",
    "            print('Initiating the prediction process using \\'pa-gb\\' model')\n",
    "            \n",
    "            regressor = Regressor_GradientBoost(X_test, y_test)\n",
    "            prediction = regressor.predict(X_test, f0, reg_gb_models)\n",
    "            rmse = regressor.rmse(y_test, prediction)\n",
    "            \n",
    "            print('Prediction completed')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('modeller.py Model_Predictor.predict_regressor_gradientboost(): ',e)\n",
    "            \n",
    "        print(\"The RMSE for the Regressor Gradient Boost model: \", rmse)\n",
    "        return prediction, rmse   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating the prediction process using 'pa-gb' model\n",
      "Prediction completed\n",
      "The RMSE for the Regressor Gradient Boost model:  5.051405898852547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-1.5589667 , 20.86878526,  7.82935102, ...,  2.32709653,\n",
       "         0.53741474, -6.44573397]), 5.051405898852547)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_regressor_gradientboost(X_test, y_test, f0, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
