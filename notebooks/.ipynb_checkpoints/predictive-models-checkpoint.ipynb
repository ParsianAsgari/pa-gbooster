{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this segment, two models are generated:\n",
    "\n",
    "1. A model using XGBoost using one-hold-out cross-validation\n",
    "2. A model using my Regressor_GradientBoost using one-hold-out cross-validation\n",
    "\n",
    "\n",
    "References: \n",
    "1. https://www.kaggle.com/omarito/gridsearchcv-xgbregressor-0-556-lb\n",
    "2. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "3. https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
    "4. https://aiinpractice.com/xgboost-hyperparameter-tuning-with-bayesian-optimization/\n",
    "5. https://github.com/fmfn/BayesianOptimization\n",
    "6. https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "1. [Data Import & Pre-Processing](#LoadingData)\n",
    "2. [XGBoost Model](#XGBoostModel)\n",
    "3. [DIY Gradient Boosting](#DIYGradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import xgboost as xgb\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.data.make_dataset import Make_DataSet\n",
    "from src.my_ml_tools.gradient_booster import * # Parsian's implementation of Regressor Gradient Boosting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data  <a class=\"anchor\" id=\"LoadingData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set path to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentdir = os.path.dirname(os.path.abspath('..'))\n",
    "datadir = os.path.join(parentdir,'data','raw')\n",
    "dataprocessed = os.path.join(parentdir,'data','processed')\n",
    "models = os.path.join(parentdir, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CSV and stitch into a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to stiched the .csv files into one dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitcher = CSV_Stitcher(datadir, dataprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following .csv files will get stitched: \n",
      "['1_record_diast.csv', '2_record_diast.csv', '3_record_diast.csv', '4_record_diast.csv', '5_record_diast.csv', '6_record_diast.csv', '7_record_diast.csv', '8_record_diast.csv', '9_record_diast.csv', '10_record_diast.csv', '11_record_diast.csv', '12_record_diast.csv', '13_record_diast.csv', '14_record_diast.csv', '15_record_diast.csv']\n",
      "Stitching is done!\n"
     ]
    }
   ],
   "source": [
    "stitched_df = stitcher.stitch_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling Summary:\n",
    "\n",
    "Here are the results of the profiling done under the 'exploratory-data-analysis' JupyterNotebook:\n",
    "\n",
    "* In total there are 66 columns and 636,984 rows.\n",
    "* There are missing values in six columns:\n",
    "\t\n",
    "| Columns    | Zero Values\t| Missing Values  | % of Total Values\t| Total Zero Missing Values | % Total Zero Missing Values| Data Type\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| feature_10    | \t0    | \t69566    | \t10.9    | \t69566    | \t10.9    | \tfloat64    | \n",
    "| feature_62    | \t0    | \t69566    | \t10.9    | \t69566    | \t10.9    | \tfloat64    | \n",
    "| feature_36    | \t0    | \t37907    | \t6.0    | \t37907    | \t6.0    | \tfloat64    | \n",
    "| feature_23    | \t0    | \t34567    | \t5.4    | \t34567    | \t5.4    | \tfloat64    | \n",
    "| feature_49    | \t0    | \t34567    | \t5.4    | \t34567    | \t5.4    | \tfloat64    | \n",
    "| feature_50    | \t0    | \t6743    | \t1.1    | \t6743    | \t1.1    | \tfloat64    | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to handle the missing data differently for the XGBoost and my own DIY Gradient Boost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitched_df = stitched_df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df_preprocessed = stitched_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.613031</td>\n",
       "      <td>-0.053620</td>\n",
       "      <td>-31.607668</td>\n",
       "      <td>-0.157781</td>\n",
       "      <td>18.810949</td>\n",
       "      <td>46.398207</td>\n",
       "      <td>-33</td>\n",
       "      <td>-14</td>\n",
       "      <td>-5.329104</td>\n",
       "      <td>46.153171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254669</td>\n",
       "      <td>0.628154</td>\n",
       "      <td>-0.446765</td>\n",
       "      <td>-0.189537</td>\n",
       "      <td>-0.072147</td>\n",
       "      <td>0.624837</td>\n",
       "      <td>-350.621060</td>\n",
       "      <td>0.925521</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.912298</td>\n",
       "      <td>-0.101502</td>\n",
       "      <td>-51.074790</td>\n",
       "      <td>0.299416</td>\n",
       "      <td>-21.747255</td>\n",
       "      <td>47.560793</td>\n",
       "      <td>-17</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.329104</td>\n",
       "      <td>4.587944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294421</td>\n",
       "      <td>0.643893</td>\n",
       "      <td>-0.230152</td>\n",
       "      <td>0.148922</td>\n",
       "      <td>-0.072147</td>\n",
       "      <td>0.062113</td>\n",
       "      <td>-295.188462</td>\n",
       "      <td>0.802615</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.129125</td>\n",
       "      <td>-0.094319</td>\n",
       "      <td>-81.653791</td>\n",
       "      <td>0.102004</td>\n",
       "      <td>-29.935130</td>\n",
       "      <td>45.707137</td>\n",
       "      <td>-16</td>\n",
       "      <td>-77</td>\n",
       "      <td>-3.881140</td>\n",
       "      <td>54.118507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.405271</td>\n",
       "      <td>0.618798</td>\n",
       "      <td>-0.216613</td>\n",
       "      <td>-1.042451</td>\n",
       "      <td>-0.052544</td>\n",
       "      <td>0.732674</td>\n",
       "      <td>146.479143</td>\n",
       "      <td>1.145548</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.928182</td>\n",
       "      <td>-0.112184</td>\n",
       "      <td>-64.922236</td>\n",
       "      <td>0.288529</td>\n",
       "      <td>-26.148199</td>\n",
       "      <td>46.367224</td>\n",
       "      <td>-27</td>\n",
       "      <td>3</td>\n",
       "      <td>-5.329104</td>\n",
       "      <td>30.266565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.354003</td>\n",
       "      <td>0.627734</td>\n",
       "      <td>-0.365535</td>\n",
       "      <td>0.040615</td>\n",
       "      <td>-0.072147</td>\n",
       "      <td>0.409759</td>\n",
       "      <td>-637.510898</td>\n",
       "      <td>0.277151</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.000687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.154080</td>\n",
       "      <td>-0.025889</td>\n",
       "      <td>-23.782382</td>\n",
       "      <td>0.043226</td>\n",
       "      <td>-10.377502</td>\n",
       "      <td>54.084277</td>\n",
       "      <td>-126</td>\n",
       "      <td>-40</td>\n",
       "      <td>-1.599500</td>\n",
       "      <td>240.520366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140494</td>\n",
       "      <td>0.732210</td>\n",
       "      <td>-1.705829</td>\n",
       "      <td>-0.541533</td>\n",
       "      <td>-0.021655</td>\n",
       "      <td>3.256243</td>\n",
       "      <td>-2415.396347</td>\n",
       "      <td>0.179903</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.000687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0 -0.613031  -0.053620 -31.607668  -0.157781  18.810949  46.398207        -33   \n",
       "1  2.912298  -0.101502 -51.074790   0.299416 -21.747255  47.560793        -17   \n",
       "2  6.129125  -0.094319 -81.653791   0.102004 -29.935130  45.707137        -16   \n",
       "3  2.928182  -0.112184 -64.922236   0.288529 -26.148199  46.367224        -27   \n",
       "4  0.154080  -0.025889 -23.782382   0.043226 -10.377502  54.084277       -126   \n",
       "\n",
       "   feature_7  feature_8   feature_9  ...  feature_56  feature_57  feature_58  \\\n",
       "0        -14  -5.329104   46.153171  ...    0.254669    0.628154   -0.446765   \n",
       "1         11  -5.329104    4.587944  ...   -0.294421    0.643893   -0.230152   \n",
       "2        -77  -3.881140   54.118507  ...   -0.405271    0.618798   -0.216613   \n",
       "3          3  -5.329104   30.266565  ...   -0.354003    0.627734   -0.365535   \n",
       "4        -40  -1.599500  240.520366  ...   -0.140494    0.732210   -1.705829   \n",
       "\n",
       "   feature_59  feature_60  feature_61   feature_62  feature_63  feature_64  \\\n",
       "0   -0.189537   -0.072147    0.624837  -350.621060    0.925521    0.000201   \n",
       "1    0.148922   -0.072147    0.062113  -295.188462    0.802615   -0.000074   \n",
       "2   -1.042451   -0.052544    0.732674   146.479143    1.145548   -0.000089   \n",
       "3    0.040615   -0.072147    0.409759  -637.510898    0.277151    0.001875   \n",
       "4   -0.541533   -0.021655    3.256243 -2415.396347    0.179903    0.000981   \n",
       "\n",
       "   feature_65  \n",
       "0    0.000106  \n",
       "1   -0.000264  \n",
       "2    0.000053  \n",
       "3    0.000687  \n",
       "4    0.000687  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate Features and Target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_analysis_df_preprocessed = analysis_df_preprocessed[['target']]\n",
    "X_analysis_df_preprocessed = analysis_df_preprocessed.drop(columns=['target'], axis=1)\n",
    "feature_list = X_analysis_df_preprocessed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to threat the missing data for each of the models differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model:  <a class=\"anchor\" id=\"XGBoostModel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data:\n",
    "\n",
    "According to the documentation, this algorithm can handle missing data internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_analysis_df_preprocessed, y_analysis_df_preprocessed, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Matrix for Scikit-learn's Cross Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X_analysis_df_preprocessed,label=y_analysis_df_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement XGRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgb_regressor(X, y, colsample_bytree=0.3, learning_rate = 0.1, \n",
    "                      max_depth = 5, alpha = 10, n_estimators = 70, nthread=-1):\n",
    "    '''\n",
    "    Fits Scikit-Learn's XGBooster Regressor to the data. Returns model for One Hold Out validation.\n",
    "    '''\n",
    "    try:\n",
    "\n",
    "        cv_results = pd.DataFrame()\n",
    "        xg_reg = xgb.XGBRegressor(colsample_bytree = colsample_bytree, learning_rate = learning_rate,\n",
    "                                  max_depth = max_depth, alpha = alpha, n_estimators = n_estimators, nthread=-1)\n",
    "        model = xg_reg.fit(X, y)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def predict_rmse_rsqured(model, Xtest, ytest):\n",
    "    '''\n",
    "    Use the input model to predict y for a given xtest, in additon it calculates RMSE\n",
    "    between ytest and predictions.\n",
    "    '''\n",
    "    try:\n",
    "        prediction = model.predict(Xtest)\n",
    "        rmse = np.sqrt(metrics.mean_squared_error(ytest, prediction))\n",
    "        r_squared = metrics.r2_score(ytest, prediction)\n",
    "        print(\"RMSE: %f\" % (rmse))\n",
    "        print(\"R Squared: %f\" % (r_squared))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return prediction, rmse, r_squared\n",
    "\n",
    "\n",
    "\n",
    "def cv_xgboost_regressor(data_matrix, params, nfold=3, num_boost_round=70, \n",
    "                         early_stopping_rounds=10, metrics=\"rmse\", seed=123):\n",
    "    cv_results = xgb.cv(dtrain=data_matrix, params=params, nfold=nfold, \n",
    "                        num_boost_round=num_boost_round, early_stopping_rounds=early_stopping_rounds, \n",
    "                        metrics=metrics, as_pandas=True, seed=seed)\n",
    "    \n",
    "    print('Top 5 Cross Validation RMSEs: ', cv_results.head())\n",
    "    print('Last Cross Validation RMSE between Validation and Actual: ', (cv_results['test-rmse-mean']).tail(1))\n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a XGBoost Regressor:\n",
    "\n",
    "#### One Hold Valdiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:21:15] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "xgb_reg_oho = fit_xgb_regressor(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.430355\n",
      "R Squared: 0.892470\n"
     ]
    }
   ],
   "source": [
    "predictions, rmse, r_squared = predict_rmse_rsqured(xgb_reg_oho, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial model can explain ~ 89% of the data with n_estimators = 70."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our data profiling, it was found that the range of our 'target' column is:\n",
    "\n",
    "- min(target) = -38.789069\n",
    "- max(target) = 41.215521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The One Hold Out validation for this model produce an RMSE of 4.43. Let us try K-Fold Cross Validation to get a better picture of its performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Fold Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:57:30] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[19:57:34] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[19:57:39] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Top 5 Cross Validation RMSEs:     train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0        12.504210        0.004915       12.505190       0.009238\n",
      "1        11.701445        0.081792       11.702601       0.073075\n",
      "2        10.928425        0.044640       10.930200       0.034758\n",
      "3        10.287901        0.033138       10.290820       0.024481\n",
      "4         9.692686        0.050746        9.696625       0.044029\n",
      "Last Cross Validation RMSE between Validation and Actual:  69    4.425063\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.504210</td>\n",
       "      <td>0.004915</td>\n",
       "      <td>12.505190</td>\n",
       "      <td>0.009238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.701445</td>\n",
       "      <td>0.081792</td>\n",
       "      <td>11.702601</td>\n",
       "      <td>0.073075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.928425</td>\n",
       "      <td>0.044640</td>\n",
       "      <td>10.930200</td>\n",
       "      <td>0.034758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.287901</td>\n",
       "      <td>0.033138</td>\n",
       "      <td>10.290820</td>\n",
       "      <td>0.024481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.692686</td>\n",
       "      <td>0.050746</td>\n",
       "      <td>9.696625</td>\n",
       "      <td>0.044029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.164222</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>9.169463</td>\n",
       "      <td>0.036304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.724850</td>\n",
       "      <td>0.071716</td>\n",
       "      <td>8.730798</td>\n",
       "      <td>0.071269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.310661</td>\n",
       "      <td>0.045305</td>\n",
       "      <td>8.317111</td>\n",
       "      <td>0.046158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.977439</td>\n",
       "      <td>0.069832</td>\n",
       "      <td>7.984037</td>\n",
       "      <td>0.069572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.646682</td>\n",
       "      <td>0.054651</td>\n",
       "      <td>7.653600</td>\n",
       "      <td>0.054678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.405196</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>7.412858</td>\n",
       "      <td>0.069859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.171480</td>\n",
       "      <td>0.044304</td>\n",
       "      <td>7.179518</td>\n",
       "      <td>0.043965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.952350</td>\n",
       "      <td>0.036094</td>\n",
       "      <td>6.961573</td>\n",
       "      <td>0.033865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.760771</td>\n",
       "      <td>0.048105</td>\n",
       "      <td>6.770262</td>\n",
       "      <td>0.045023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.592921</td>\n",
       "      <td>0.038885</td>\n",
       "      <td>6.603078</td>\n",
       "      <td>0.033999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.453544</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>6.464091</td>\n",
       "      <td>0.028866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.331185</td>\n",
       "      <td>0.039265</td>\n",
       "      <td>6.342286</td>\n",
       "      <td>0.031342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.226995</td>\n",
       "      <td>0.041961</td>\n",
       "      <td>6.238879</td>\n",
       "      <td>0.034117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.120738</td>\n",
       "      <td>0.035960</td>\n",
       "      <td>6.132851</td>\n",
       "      <td>0.027068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.020960</td>\n",
       "      <td>0.028147</td>\n",
       "      <td>6.033577</td>\n",
       "      <td>0.019065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.928477</td>\n",
       "      <td>0.021144</td>\n",
       "      <td>5.940865</td>\n",
       "      <td>0.012039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.845851</td>\n",
       "      <td>0.018189</td>\n",
       "      <td>5.858716</td>\n",
       "      <td>0.009136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.764133</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>5.777401</td>\n",
       "      <td>0.018779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.699608</td>\n",
       "      <td>0.019468</td>\n",
       "      <td>5.713686</td>\n",
       "      <td>0.017833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.635105</td>\n",
       "      <td>0.024755</td>\n",
       "      <td>5.649578</td>\n",
       "      <td>0.022830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.570357</td>\n",
       "      <td>0.035072</td>\n",
       "      <td>5.585045</td>\n",
       "      <td>0.031053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.510351</td>\n",
       "      <td>0.034363</td>\n",
       "      <td>5.525133</td>\n",
       "      <td>0.030948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.449112</td>\n",
       "      <td>0.036810</td>\n",
       "      <td>5.464591</td>\n",
       "      <td>0.032709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.408748</td>\n",
       "      <td>0.036821</td>\n",
       "      <td>5.424519</td>\n",
       "      <td>0.033054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.356280</td>\n",
       "      <td>0.035095</td>\n",
       "      <td>5.372656</td>\n",
       "      <td>0.030280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4.938552</td>\n",
       "      <td>0.026051</td>\n",
       "      <td>4.959134</td>\n",
       "      <td>0.028239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.913971</td>\n",
       "      <td>0.024509</td>\n",
       "      <td>4.934786</td>\n",
       "      <td>0.025411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.891535</td>\n",
       "      <td>0.022997</td>\n",
       "      <td>4.912113</td>\n",
       "      <td>0.022613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.866997</td>\n",
       "      <td>0.028371</td>\n",
       "      <td>4.888056</td>\n",
       "      <td>0.027899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4.842504</td>\n",
       "      <td>0.024180</td>\n",
       "      <td>4.864192</td>\n",
       "      <td>0.024801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.828454</td>\n",
       "      <td>0.025113</td>\n",
       "      <td>4.850477</td>\n",
       "      <td>0.025371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.803511</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>4.825393</td>\n",
       "      <td>0.016847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.785260</td>\n",
       "      <td>0.019801</td>\n",
       "      <td>4.807610</td>\n",
       "      <td>0.019403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4.758966</td>\n",
       "      <td>0.018213</td>\n",
       "      <td>4.781638</td>\n",
       "      <td>0.019673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.739402</td>\n",
       "      <td>0.015758</td>\n",
       "      <td>4.761937</td>\n",
       "      <td>0.016019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4.716171</td>\n",
       "      <td>0.017203</td>\n",
       "      <td>4.738826</td>\n",
       "      <td>0.017814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4.695949</td>\n",
       "      <td>0.020379</td>\n",
       "      <td>4.719170</td>\n",
       "      <td>0.022431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4.672157</td>\n",
       "      <td>0.018977</td>\n",
       "      <td>4.695452</td>\n",
       "      <td>0.020852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4.653535</td>\n",
       "      <td>0.015679</td>\n",
       "      <td>4.676788</td>\n",
       "      <td>0.018705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4.636373</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>4.659840</td>\n",
       "      <td>0.020401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4.622634</td>\n",
       "      <td>0.022407</td>\n",
       "      <td>4.646258</td>\n",
       "      <td>0.024917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>4.601171</td>\n",
       "      <td>0.029028</td>\n",
       "      <td>4.624941</td>\n",
       "      <td>0.031151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4.583547</td>\n",
       "      <td>0.031482</td>\n",
       "      <td>4.607291</td>\n",
       "      <td>0.031668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4.563421</td>\n",
       "      <td>0.032212</td>\n",
       "      <td>4.587222</td>\n",
       "      <td>0.032196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4.545508</td>\n",
       "      <td>0.026261</td>\n",
       "      <td>4.569403</td>\n",
       "      <td>0.026571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4.529500</td>\n",
       "      <td>0.029886</td>\n",
       "      <td>4.553245</td>\n",
       "      <td>0.030541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4.511908</td>\n",
       "      <td>0.029002</td>\n",
       "      <td>4.536335</td>\n",
       "      <td>0.029437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4.497378</td>\n",
       "      <td>0.025656</td>\n",
       "      <td>4.522169</td>\n",
       "      <td>0.025177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4.487247</td>\n",
       "      <td>0.026865</td>\n",
       "      <td>4.512152</td>\n",
       "      <td>0.026464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4.477565</td>\n",
       "      <td>0.028759</td>\n",
       "      <td>4.502445</td>\n",
       "      <td>0.027963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>4.458982</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>4.484239</td>\n",
       "      <td>0.019361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4.443309</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>4.468828</td>\n",
       "      <td>0.018714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>4.432897</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>4.458845</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4.414127</td>\n",
       "      <td>0.024449</td>\n",
       "      <td>4.440496</td>\n",
       "      <td>0.026132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4.398532</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>4.425063</td>\n",
       "      <td>0.025738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "0         12.504210        0.004915       12.505190       0.009238\n",
       "1         11.701445        0.081792       11.702601       0.073075\n",
       "2         10.928425        0.044640       10.930200       0.034758\n",
       "3         10.287901        0.033138       10.290820       0.024481\n",
       "4          9.692686        0.050746        9.696625       0.044029\n",
       "5          9.164222        0.040837        9.169463       0.036304\n",
       "6          8.724850        0.071716        8.730798       0.071269\n",
       "7          8.310661        0.045305        8.317111       0.046158\n",
       "8          7.977439        0.069832        7.984037       0.069572\n",
       "9          7.646682        0.054651        7.653600       0.054678\n",
       "10         7.405196        0.069721        7.412858       0.069859\n",
       "11         7.171480        0.044304        7.179518       0.043965\n",
       "12         6.952350        0.036094        6.961573       0.033865\n",
       "13         6.760771        0.048105        6.770262       0.045023\n",
       "14         6.592921        0.038885        6.603078       0.033999\n",
       "15         6.453544        0.035714        6.464091       0.028866\n",
       "16         6.331185        0.039265        6.342286       0.031342\n",
       "17         6.226995        0.041961        6.238879       0.034117\n",
       "18         6.120738        0.035960        6.132851       0.027068\n",
       "19         6.020960        0.028147        6.033577       0.019065\n",
       "20         5.928477        0.021144        5.940865       0.012039\n",
       "21         5.845851        0.018189        5.858716       0.009136\n",
       "22         5.764133        0.023591        5.777401       0.018779\n",
       "23         5.699608        0.019468        5.713686       0.017833\n",
       "24         5.635105        0.024755        5.649578       0.022830\n",
       "25         5.570357        0.035072        5.585045       0.031053\n",
       "26         5.510351        0.034363        5.525133       0.030948\n",
       "27         5.449112        0.036810        5.464591       0.032709\n",
       "28         5.408748        0.036821        5.424519       0.033054\n",
       "29         5.356280        0.035095        5.372656       0.030280\n",
       "..              ...             ...             ...            ...\n",
       "40         4.938552        0.026051        4.959134       0.028239\n",
       "41         4.913971        0.024509        4.934786       0.025411\n",
       "42         4.891535        0.022997        4.912113       0.022613\n",
       "43         4.866997        0.028371        4.888056       0.027899\n",
       "44         4.842504        0.024180        4.864192       0.024801\n",
       "45         4.828454        0.025113        4.850477       0.025371\n",
       "46         4.803511        0.017705        4.825393       0.016847\n",
       "47         4.785260        0.019801        4.807610       0.019403\n",
       "48         4.758966        0.018213        4.781638       0.019673\n",
       "49         4.739402        0.015758        4.761937       0.016019\n",
       "50         4.716171        0.017203        4.738826       0.017814\n",
       "51         4.695949        0.020379        4.719170       0.022431\n",
       "52         4.672157        0.018977        4.695452       0.020852\n",
       "53         4.653535        0.015679        4.676788       0.018705\n",
       "54         4.636373        0.017613        4.659840       0.020401\n",
       "55         4.622634        0.022407        4.646258       0.024917\n",
       "56         4.601171        0.029028        4.624941       0.031151\n",
       "57         4.583547        0.031482        4.607291       0.031668\n",
       "58         4.563421        0.032212        4.587222       0.032196\n",
       "59         4.545508        0.026261        4.569403       0.026571\n",
       "60         4.529500        0.029886        4.553245       0.030541\n",
       "61         4.511908        0.029002        4.536335       0.029437\n",
       "62         4.497378        0.025656        4.522169       0.025177\n",
       "63         4.487247        0.026865        4.512152       0.026464\n",
       "64         4.477565        0.028759        4.502445       0.027963\n",
       "65         4.458982        0.019346        4.484239       0.019361\n",
       "66         4.443309        0.017648        4.468828       0.018714\n",
       "67         4.432897        0.017700        4.458845       0.019500\n",
       "68         4.414127        0.024449        4.440496       0.026132\n",
       "69         4.398532        0.023289        4.425063       0.025738\n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Parameters for Cross Validation:\n",
    "\n",
    "cv_param_dic = {'colsample_bytree': 0.3,\n",
    "                'learning_rate': 0.1,\n",
    "                'max_depth': 5, \n",
    "                'alpha': 10}\n",
    "\n",
    "cv_results = cv_xgboost_regressor(data_matrix=data_dmatrix, params=cv_param_dic)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate= 0.1 and num_boost_round = 50, The lowest RMSE base on 3 Fold Cross Validation is 4.76.\n",
    "learning rate = 0.1 and num_boost_round = 70, The lowest RMSE base on 3 Fold Cross Validation is 4.42\n",
    "\n",
    "learning rate = 0.15 and num_boost_round = 50, The lowest RMSE base on 3 Fold Cross Validation is 4.36\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter space will be optimized using a Bayesian Optimization technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n",
    "test_dmatrix = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_evaluate(max_depth, gamma, colsample_bytree):\n",
    "    params = {'eval_metric': 'rmse',\n",
    "              'max_depth': int(max_depth),\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.1,\n",
    "              'gamma': gamma,\n",
    "              'colsample_bytree': colsample_bytree}\n",
    "    # Used around 1000 boosting rounds in the full model\n",
    "    cv_result = xgb.cv(params, train_dmatrix, num_boost_round=100, nfold=3)    \n",
    "    \n",
    "    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n",
    "    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo_training = BayesianOptimization(xgb_evaluate, {'max_depth': (3,5), \n",
    "                                                      'gamma': (0, 1),\n",
    "                                                      'colsample_bytree': (0.3, 0.5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-5.211   \u001b[0m | \u001b[0m 0.3616  \u001b[0m | \u001b[0m 0.8449  \u001b[0m | \u001b[0m 3.128   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-4.547   \u001b[0m | \u001b[95m 0.3935  \u001b[0m | \u001b[95m 0.4583  \u001b[0m | \u001b[95m 4.21    \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-4.572   \u001b[0m | \u001b[0m 0.3689  \u001b[0m | \u001b[0m 0.7059  \u001b[0m | \u001b[0m 4.935   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-4.544   \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 4.891   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-4.604   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 4.382   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-4.544   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.4849  \u001b[0m | \u001b[0m 4.527   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-4.604   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.375   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-4.054   \u001b[0m | \u001b[95m 0.3     \u001b[0m | \u001b[95m 0.1562  \u001b[0m | \u001b[95m 5.0     \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "xgb_bo_training.maximize(init_points=3, n_iter=5, acq='ei')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo_training_2 = BayesianOptimization(xgb_evaluate, {'max_depth': (3,7), \n",
    "                                                      'gamma': (0, 1),\n",
    "                                                      'colsample_bytree': (0.3, 0.9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-3.985   \u001b[0m | \u001b[0m 0.7615  \u001b[0m | \u001b[0m 0.9304  \u001b[0m | \u001b[0m 5.691   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-3.973   \u001b[0m | \u001b[95m 0.7157  \u001b[0m | \u001b[95m 0.7656  \u001b[0m | \u001b[95m 5.152   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-3.987   \u001b[0m | \u001b[0m 0.8959  \u001b[0m | \u001b[0m 0.3682  \u001b[0m | \u001b[0m 5.359   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-5.224   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m-3.13    \u001b[0m | \u001b[95m 0.3     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 7.0     \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m-3.125   \u001b[0m | \u001b[95m 0.9     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 7.0     \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-3.137   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 7.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-3.143   \u001b[0m | \u001b[0m 0.5537  \u001b[0m | \u001b[0m 0.4688  \u001b[0m | \u001b[0m 7.0     \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "xgb_bo_training_2.maximize(init_points=3, n_iter=5, acq='ei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9, 'gamma': 0.0, 'max_depth': 7}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_params = xgb_bo_training_2.max['params']\n",
    "optimized_params['max_depth'] = int(optimized_params['max_depth'])\n",
    "optimized_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimized_params = xgb.train(optimized_params, train_dmatrix, num_boost_round=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction on Y Test Data RMSE: 1.8790727643245035\n",
      "Model Prediction on Y Train Data RMSE: 1.650457238010561\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb = model_optimized_params.predict(test_dmatrix)\n",
    "y_train_pred_xgb = model_optimized_params.predict(train_dmatrix)\n",
    "\n",
    "print('Model Prediction on Y Test Data RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_xgb)))\n",
    "print('Model Prediction on Y Train Data RMSE:', np.sqrt(metrics.mean_squared_error(y_train, y_train_pred_xgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_fname = \"XGBoost_Regressor_Model_Pickled.pkl\"\n",
    "\n",
    "\n",
    "with open(os.path.join(models, \"XGBoost_Regressor_Model_Pickled.pkl\"), 'wb') as file:\n",
    "    pickle.dump(model_optimized_params, file)\n",
    "\n",
    "## test loading:\n",
    "\n",
    "#with open(os.path.join(models, \"XGBoost_Regressor_Model_Pickled.pkl\"), 'rb') as file:\n",
    "#    pickled_xgb_optimized_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY Gradient Boosting Regressor  <a class=\"anchor\" id=\"DIYGradientBoostingRegressor\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data:\n",
    "\n",
    "This model cannot operate with missing data. I am going to fill the missing data with each column's median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_stiched_df = stitched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_10',\n",
       " 'feature_23',\n",
       " 'feature_36',\n",
       " 'feature_49',\n",
       " 'feature_50',\n",
       " 'feature_62']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_value_columns = processed_stiched_df.columns[processed_stiched_df.isnull().any()].tolist()\n",
    "missing_value_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_stiched_df = processed_stiched_df.apply(lambda x: x.fillna(x.median()), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_regressor = Regressor_GradientBoost(target_col='target', \n",
    "                                       feature_cols = feature_list, \n",
    "                                       max_depth=5, \n",
    "                                       ntrees=70, \n",
    "                                       learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_tst, y_tr, y_tst = my_regressor.load_split_data(processed_stiched_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Regressor_GradientBoost model:\n",
    "\n",
    "Fit the model using the model's decision tree method (Based on Scikit-learn 's DecisionTreeRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = my_regressor.fit_decisiontree(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss at first prediction:  91.3721086792508\n",
      "Mean loss for tree #0 is: 77.97227766673323 The training set R Squared is: 0.146651217819192\n",
      "Mean loss for tree #1 is: 67.06629550462937 The training set R Squared is: 0.2660091085338573\n",
      "Mean loss for tree #2 is: 58.15373029431931 The training set R Squared is: 0.36355052832960455\n",
      "Mean loss for tree #3 is: 50.86731047812349 The training set R Squared is: 0.44329499216567114\n",
      "Mean loss for tree #4 is: 44.880399497674965 The training set R Squared is: 0.508817295054199\n",
      "Mean loss for tree #5 is: 39.985873960062555 The training set R Squared is: 0.5623842490006905\n",
      "Mean loss for tree #6 is: 35.89055328404571 The training set R Squared is: 0.6072044981468867\n",
      "Mean loss for tree #7 is: 32.50124245838794 The training set R Squared is: 0.6442979928100572\n",
      "Mean loss for tree #8 is: 29.65741419050553 The training set R Squared is: 0.6754215852168473\n",
      "Mean loss for tree #9 is: 27.274533260209914 The training set R Squared is: 0.7015004506905624\n",
      "Mean loss for tree #10 is: 25.253495852421125 The training set R Squared is: 0.7236192070266273\n",
      "Mean loss for tree #11 is: 23.636519161913466 The training set R Squared is: 0.7413158183216941\n",
      "Mean loss for tree #12 is: 22.261623921449555 The training set R Squared is: 0.7563630275886967\n",
      "Mean loss for tree #13 is: 21.09335176331141 The training set R Squared is: 0.7691489003788164\n",
      "Mean loss for tree #14 is: 20.04475021280098 The training set R Squared is: 0.7806250670741892\n",
      "Mean loss for tree #15 is: 19.21134395257718 The training set R Squared is: 0.789746080830689\n",
      "Mean loss for tree #16 is: 18.416733005297353 The training set R Squared is: 0.79844250864401\n",
      "Mean loss for tree #17 is: 17.782903030315964 The training set R Squared is: 0.8053793078942753\n",
      "Mean loss for tree #18 is: 17.189623578967854 The training set R Squared is: 0.811872311721411\n",
      "Mean loss for tree #19 is: 16.66903335062419 The training set R Squared is: 0.8175697858835869\n",
      "Mean loss for tree #20 is: 16.235048827741664 The training set R Squared is: 0.8223194247958896\n",
      "Mean loss for tree #21 is: 15.82726473117566 The training set R Squared is: 0.8267823194632135\n",
      "Mean loss for tree #22 is: 15.48004455746113 The training set R Squared is: 0.8305823868878703\n",
      "Mean loss for tree #23 is: 15.146331499649337 The training set R Squared is: 0.8342346289411098\n",
      "Mean loss for tree #24 is: 14.766598554797284 The training set R Squared is: 0.838390524545806\n",
      "Mean loss for tree #25 is: 14.522862768962797 The training set R Squared is: 0.8410580320528338\n",
      "Mean loss for tree #26 is: 14.228813924443363 The training set R Squared is: 0.8442761786926526\n",
      "Mean loss for tree #27 is: 14.008788354709722 The training set R Squared is: 0.8466841954596284\n",
      "Mean loss for tree #28 is: 13.80175328085178 The training set R Squared is: 0.8489500408784403\n",
      "Mean loss for tree #29 is: 13.51760261534493 The training set R Squared is: 0.8520598592859786\n",
      "Mean loss for tree #30 is: 13.343738151405129 The training set R Squared is: 0.8539626769669239\n",
      "Mean loss for tree #31 is: 13.079621189294796 The training set R Squared is: 0.8568532413407594\n",
      "Mean loss for tree #32 is: 12.927715140709864 The training set R Squared is: 0.8585157404422978\n",
      "Mean loss for tree #33 is: 12.730077792303977 The training set R Squared is: 0.8606787347220911\n",
      "Mean loss for tree #34 is: 12.61252256118844 The training set R Squared is: 0.8619652895889427\n",
      "Mean loss for tree #35 is: 12.422305067652903 The training set R Squared is: 0.864047079057145\n",
      "Mean loss for tree #36 is: 12.29194473281182 The training set R Squared is: 0.8654737762924924\n",
      "Mean loss for tree #37 is: 12.110579641755079 The training set R Squared is: 0.8674586827773931\n",
      "Mean loss for tree #38 is: 11.980038262671863 The training set R Squared is: 0.8688873614077852\n",
      "Mean loss for tree #39 is: 11.749493309632632 The training set R Squared is: 0.871410505027555\n",
      "Mean loss for tree #40 is: 11.627998950556584 The training set R Squared is: 0.8727401707300546\n",
      "Mean loss for tree #41 is: 11.565364309337781 The training set R Squared is: 0.8734256604503261\n",
      "Mean loss for tree #42 is: 11.287724815549737 The training set R Squared is: 0.8764642189098024\n",
      "Mean loss for tree #43 is: 11.070235910629801 The training set R Squared is: 0.8788444737606959\n",
      "Mean loss for tree #44 is: 11.015579490791565 The training set R Squared is: 0.8794426477618\n",
      "Mean loss for tree #45 is: 10.859694827717236 The training set R Squared is: 0.8811486898497785\n",
      "Mean loss for tree #46 is: 10.663967177613978 The training set R Squared is: 0.8832907838972154\n",
      "Mean loss for tree #47 is: 10.520565640710304 The training set R Squared is: 0.88486020742236\n",
      "Mean loss for tree #48 is: 10.44393715299838 The training set R Squared is: 0.8856988494195726\n",
      "Mean loss for tree #49 is: 10.312599600999933 The training set R Squared is: 0.8871362415723479\n",
      "Mean loss for tree #50 is: 10.236604186880315 The training set R Squared is: 0.8879679550483588\n",
      "Mean loss for tree #51 is: 10.19107381025984 The training set R Squared is: 0.8884662512711169\n",
      "Mean loss for tree #52 is: 10.148620992489548 The training set R Squared is: 0.8889308658935002\n",
      "Mean loss for tree #53 is: 10.050048354321062 The training set R Squared is: 0.8900096703513714\n",
      "Mean loss for tree #54 is: 9.923138962480524 The training set R Squared is: 0.891398599573594\n",
      "Mean loss for tree #55 is: 9.856844726628049 The training set R Squared is: 0.8921241408444572\n",
      "Mean loss for tree #56 is: 9.793895209830362 The training set R Squared is: 0.8928130766445325\n",
      "Mean loss for tree #57 is: 9.667092229737788 The training set R Squared is: 0.894200841269053\n",
      "Mean loss for tree #58 is: 9.56820237600607 The training set R Squared is: 0.8952831174161272\n",
      "Mean loss for tree #59 is: 9.467135387574562 The training set R Squared is: 0.8963892206886944\n",
      "Mean loss for tree #60 is: 9.312246907353991 The training set R Squared is: 0.8980843602937569\n",
      "Mean loss for tree #61 is: 9.239187468311766 The training set R Squared is: 0.8988839416988355\n",
      "Mean loss for tree #62 is: 9.166582444665185 The training set R Squared is: 0.8996785498642366\n",
      "Mean loss for tree #63 is: 9.079898692482164 The training set R Squared is: 0.9006272392776256\n",
      "Mean loss for tree #64 is: 9.014587258199121 The training set R Squared is: 0.9013420245138082\n",
      "Mean loss for tree #65 is: 8.977826138142474 The training set R Squared is: 0.9017443477236802\n",
      "Mean loss for tree #66 is: 8.92776826244797 The training set R Squared is: 0.9022921940678049\n",
      "Mean loss for tree #67 is: 8.840819198193879 The training set R Squared is: 0.9032437871251479\n",
      "Mean loss for tree #68 is: 8.790339642488346 The training set R Squared is: 0.9037962484444186\n",
      "Mean loss for tree #69 is: 8.723999005546254 The training set R Squared is: 0.9045222975408123\n"
     ]
    }
   ],
   "source": [
    "f0, models, training_predictions, training_means_sq_err, training_r_squared  = my_regressor.boost_gradient(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_regressor.predict(X_tst, f0, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Squared Mean: 47.65400273124723 R Squared:  0.4770418033609244\n"
     ]
    }
   ],
   "source": [
    "loss, r_squared = my_regressor.loss_r_squared(y_tst, y_pred)\n",
    "\n",
    "print('Loss Squared Mean:', loss.mean(), 'R Squared: ', r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_regressor.ntrees = 20\n",
    "my_regressor.max_depth=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model_2 = my_regressor.fit_decisiontree(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss at first prediction:  91.3721086792508\n",
      "Mean loss for tree #0 is: 78.87153670368777 The training set R Squared is: 0.13680949423465527\n",
      "Mean loss for tree #1 is: 68.69520508540084 The training set R Squared is: 0.24818190060004996\n",
      "Mean loss for tree #2 is: 60.343939552109276 The training set R Squared is: 0.33958031149376533\n",
      "Mean loss for tree #3 is: 53.52307005079341 The training set R Squared is: 0.41422967222220286\n",
      "Mean loss for tree #4 is: 47.92480631692255 The training set R Squared is: 0.4754985190814308\n",
      "Mean loss for tree #5 is: 43.32592095272779 The training set R Squared is: 0.5258299104728279\n",
      "Mean loss for tree #6 is: 39.46806436052665 The training set R Squared is: 0.5680512912416893\n",
      "Mean loss for tree #7 is: 36.340034237744376 The training set R Squared is: 0.6022852622859929\n",
      "Mean loss for tree #8 is: 33.71611005495408 The training set R Squared is: 0.6310021674851535\n",
      "Mean loss for tree #9 is: 31.552756001356308 The training set R Squared is: 0.6546784740175275\n",
      "Mean loss for tree #10 is: 29.721908966266387 The training set R Squared is: 0.6747157377028362\n",
      "Mean loss for tree #11 is: 28.19714157243684 The training set R Squared is: 0.6914031865958274\n",
      "Mean loss for tree #12 is: 26.88399047397014 The training set R Squared is: 0.705774651996464\n",
      "Mean loss for tree #13 is: 25.796487009159854 The training set R Squared is: 0.7176765713078367\n",
      "Mean loss for tree #14 is: 24.848188218825303 The training set R Squared is: 0.7280549986424114\n",
      "Mean loss for tree #15 is: 24.056251275490812 The training set R Squared is: 0.7367221614646449\n",
      "Mean loss for tree #16 is: 23.39580974306558 The training set R Squared is: 0.743950204485343\n",
      "Mean loss for tree #17 is: 22.805155175522543 The training set R Squared is: 0.7504144809049376\n",
      "Mean loss for tree #18 is: 22.29350286555381 The training set R Squared is: 0.7560141361757047\n",
      "Mean loss for tree #19 is: 21.80547558385796 The training set R Squared is: 0.7613552330240934\n"
     ]
    }
   ],
   "source": [
    "f0_2, models_2, training_predictions_2, training_means_sq_err_2, training_r_squared_2  = my_regressor.boost_gradient(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = my_regressor.predict(X_tst, f0_2, models_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Squared Mean: 22.380939730764197 R Squared:  0.7543900782753485\n"
     ]
    }
   ],
   "source": [
    "loss_2, r_squared_2 = my_regressor.loss_r_squared(y_tst, y_pred_2)\n",
    "\n",
    "print('Loss Squared Mean:', loss_2.mean(), 'R Squared: ', r_squared_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss at first prediction:  91.3721086792508\n",
      "Mean loss for tree #0 is: 79.57217147960725 The training set R Squared is: 0.12914156595718906\n",
      "Mean loss for tree #1 is: 69.9397910080698 The training set R Squared is: 0.23456083022465912\n",
      "Mean loss for tree #2 is: 62.011348183545536 The training set R Squared is: 0.3213317599878871\n",
      "Mean loss for tree #3 is: 55.50225333276047 The training set R Squared is: 0.3925689782689358\n",
      "Mean loss for tree #4 is: 50.13668689114958 The training set R Squared is: 0.4512911257510036\n",
      "Mean loss for tree #5 is: 45.73893987795191 The training set R Squared is: 0.49942120698437875\n",
      "Mean loss for tree #6 is: 42.08815528447682 The training set R Squared is: 0.5393763382191225\n",
      "Mean loss for tree #7 is: 39.100893647352 The training set R Squared is: 0.572069702532434\n",
      "Mean loss for tree #8 is: 36.59117211720176 The training set R Squared is: 0.5995367443510636\n",
      "Mean loss for tree #9 is: 34.502687670471715 The training set R Squared is: 0.6223936585332833\n",
      "Mean loss for tree #10 is: 32.73121586525208 The training set R Squared is: 0.6417811043394961\n",
      "Mean loss for tree #11 is: 31.237120940798725 The training set R Squared is: 0.6581328657911172\n",
      "Mean loss for tree #12 is: 29.98798126138569 The training set R Squared is: 0.6718037736586119\n",
      "Mean loss for tree #13 is: 28.891408517747845 The training set R Squared is: 0.6838049495041452\n",
      "Mean loss for tree #14 is: 27.98170413950797 The training set R Squared is: 0.6937609896064316\n",
      "Mean loss for tree #15 is: 27.188541744224022 The training set R Squared is: 0.7024415640918866\n",
      "Mean loss for tree #16 is: 26.473320854982436 The training set R Squared is: 0.7102691265677925\n",
      "Mean loss for tree #17 is: 25.889359789154742 The training set R Squared is: 0.7166601475726497\n",
      "Mean loss for tree #18 is: 25.35177773608504 The training set R Squared is: 0.7225435846612815\n",
      "Mean loss for tree #19 is: 24.858708967341347 The training set R Squared is: 0.7279398568483936\n",
      "Mean loss for tree #20 is: 24.465029704418637 The training set R Squared is: 0.732248384566685\n",
      "Mean loss for tree #21 is: 24.050643259672967 The training set R Squared is: 0.7367835370408391\n",
      "Mean loss for tree #22 is: 23.708234348100163 The training set R Squared is: 0.7405309487676928\n",
      "Mean loss for tree #23 is: 23.430474744048812 The training set R Squared is: 0.7435708217449933\n",
      "Mean loss for tree #24 is: 23.142997277856285 The training set R Squared is: 0.746717049520041\n",
      "Mean loss for tree #25 is: 22.86546020531392 The training set R Squared is: 0.7497544870549184\n",
      "Mean loss for tree #26 is: 22.624400449201488 The training set R Squared is: 0.7523927074002345\n",
      "Mean loss for tree #27 is: 22.39303205265402 The training set R Squared is: 0.7549248629988361\n",
      "Mean loss for tree #28 is: 22.206329700753955 The training set R Squared is: 0.7569681818474207\n",
      "Mean loss for tree #29 is: 21.973068399668815 The training set R Squared is: 0.7595210538830618\n",
      "Mean loss for tree #30 is: 21.801932316423834 The training set R Squared is: 0.7613940114597131\n",
      "Mean loss for tree #31 is: 21.62931338644068 The training set R Squared is: 0.7632831976947532\n",
      "Mean loss for tree #32 is: 21.437371240736034 The training set R Squared is: 0.7653838622025334\n",
      "Mean loss for tree #33 is: 21.26205909151963 The training set R Squared is: 0.767302523725735\n",
      "Mean loss for tree #34 is: 21.127442692358546 The training set R Squared is: 0.7687758004302667\n",
      "Mean loss for tree #35 is: 20.99829965020312 The training set R Squared is: 0.7701891752994952\n",
      "Mean loss for tree #36 is: 20.87549329074507 The training set R Squared is: 0.7715331998736553\n",
      "Mean loss for tree #37 is: 20.73359912507587 The training set R Squared is: 0.7730861263379828\n",
      "Mean loss for tree #38 is: 20.611015279097224 The training set R Squared is: 0.7744277156670456\n",
      "Mean loss for tree #39 is: 20.498880446981854 The training set R Squared is: 0.7756549482847193\n",
      "Mean loss for tree #40 is: 20.405251776393303 The training set R Squared is: 0.7766796446821329\n",
      "Mean loss for tree #41 is: 20.314933364147276 The training set R Squared is: 0.7776681127557262\n",
      "Mean loss for tree #42 is: 20.154601154531395 The training set R Squared is: 0.7794228299438564\n",
      "Mean loss for tree #43 is: 20.041706042120936 The training set R Squared is: 0.78065838326579\n",
      "Mean loss for tree #44 is: 19.97100943915757 The training set R Squared is: 0.7814321051814351\n",
      "Mean loss for tree #45 is: 19.842603854207287 The training set R Squared is: 0.7828374091281949\n",
      "Mean loss for tree #46 is: 19.75277261633349 The training set R Squared is: 0.7838205454393951\n",
      "Mean loss for tree #47 is: 19.581257046376617 The training set R Squared is: 0.7856976562168042\n",
      "Mean loss for tree #48 is: 19.496583087042552 The training set R Squared is: 0.7866243499372197\n",
      "Mean loss for tree #49 is: 19.431762364153496 The training set R Squared is: 0.7873337647009379\n",
      "Mean loss for tree #50 is: 19.315487306724886 The training set R Squared is: 0.788606309015712\n",
      "Mean loss for tree #51 is: 19.249656169837714 The training set R Squared is: 0.7893267820116753\n",
      "Mean loss for tree #52 is: 19.111118143611687 The training set R Squared is: 0.7908429780175246\n",
      "Mean loss for tree #53 is: 19.036359325239985 The training set R Squared is: 0.7916611578696918\n",
      "Mean loss for tree #54 is: 18.95185926623826 The training set R Squared is: 0.7925859483798663\n",
      "Mean loss for tree #55 is: 18.88400677954 The training set R Squared is: 0.7933285435511885\n",
      "Mean loss for tree #56 is: 18.832417483299434 The training set R Squared is: 0.7938931501580273\n",
      "Mean loss for tree #57 is: 18.770114293102186 The training set R Squared is: 0.7945750123925467\n",
      "Mean loss for tree #58 is: 18.70540479245096 The training set R Squared is: 0.7952832099113165\n",
      "Mean loss for tree #59 is: 18.659777792827555 The training set R Squared is: 0.795782563601222\n",
      "Mean loss for tree #60 is: 18.5988660137732 The training set R Squared is: 0.7964491978721646\n",
      "Mean loss for tree #61 is: 18.543623992060763 The training set R Squared is: 0.7970537808517093\n",
      "Mean loss for tree #62 is: 18.501781472460497 The training set R Squared is: 0.797511716213005\n",
      "Mean loss for tree #63 is: 18.44754616055966 The training set R Squared is: 0.7981052814998904\n",
      "Mean loss for tree #64 is: 18.36601737795368 The training set R Squared is: 0.7989975535923681\n",
      "Mean loss for tree #65 is: 18.300838236556615 The training set R Squared is: 0.7997108909809788\n",
      "Mean loss for tree #66 is: 18.172676264398856 The training set R Squared is: 0.8011135287663012\n",
      "Mean loss for tree #67 is: 18.08597406727102 The training set R Squared is: 0.8020624200459332\n",
      "Mean loss for tree #68 is: 18.029907593280797 The training set R Squared is: 0.8026760260445329\n",
      "Mean loss for tree #69 is: 17.96846942218012 The training set R Squared is: 0.8033484212862405\n",
      "Mean loss for tree #70 is: 17.925169220685053 The training set R Squared is: 0.803822309895371\n",
      "Mean loss for tree #71 is: 17.877233074041758 The training set R Squared is: 0.8043469354877568\n",
      "Mean loss for tree #72 is: 17.7863757357629 The training set R Squared is: 0.8053413017072966\n",
      "Mean loss for tree #73 is: 17.755061684573374 The training set R Squared is: 0.8056840107860488\n",
      "Mean loss for tree #74 is: 17.688493409940985 The training set R Squared is: 0.8064125512082287\n",
      "Mean loss for tree #75 is: 17.647006089465407 The training set R Squared is: 0.8068665991783864\n",
      "Mean loss for tree #76 is: 17.614238908612666 The training set R Squared is: 0.8072252116841848\n",
      "Mean loss for tree #77 is: 17.5611736302476 The training set R Squared is: 0.8078059718213003\n",
      "Mean loss for tree #78 is: 17.487018441708287 The training set R Squared is: 0.8086175453923896\n",
      "Mean loss for tree #79 is: 17.447192819010265 The training set R Squared is: 0.8090534073121124\n",
      "Mean loss for tree #80 is: 17.408615958373808 The training set R Squared is: 0.8094756024567193\n",
      "Mean loss for tree #81 is: 17.379370546832256 The training set R Squared is: 0.8097956718079012\n",
      "Mean loss for tree #82 is: 17.347928909041574 The training set R Squared is: 0.8101397772274377\n",
      "Mean loss for tree #83 is: 17.316383525338637 The training set R Squared is: 0.8104850180690816\n",
      "Mean loss for tree #84 is: 17.251199428002913 The training set R Squared is: 0.8111984096967685\n",
      "Mean loss for tree #85 is: 17.194747312203493 The training set R Squared is: 0.8118162362591088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss for tree #86 is: 17.149504142217165 The training set R Squared is: 0.8123113892181447\n",
      "Mean loss for tree #87 is: 17.071558785800626 The training set R Squared is: 0.8131644433671982\n",
      "Mean loss for tree #88 is: 17.043850481874898 The training set R Squared is: 0.8134676902149183\n",
      "Mean loss for tree #89 is: 16.997883195506216 The training set R Squared is: 0.8139707680910093\n",
      "Mean loss for tree #90 is: 16.966988443050507 The training set R Squared is: 0.8143088882559297\n",
      "Mean loss for tree #91 is: 16.944641095906356 The training set R Squared is: 0.8145534634054664\n",
      "Mean loss for tree #92 is: 16.91150373504358 The training set R Squared is: 0.8149161272570731\n",
      "Mean loss for tree #93 is: 16.85138998717712 The training set R Squared is: 0.8155740276682084\n",
      "Mean loss for tree #94 is: 16.82737634945258 The training set R Squared is: 0.8158368391330195\n",
      "Mean loss for tree #95 is: 16.80362626158567 The training set R Squared is: 0.816096766239984\n",
      "Mean loss for tree #96 is: 16.76378199348035 The training set R Squared is: 0.8165328322198782\n",
      "Mean loss for tree #97 is: 16.690720928517525 The training set R Squared is: 0.8173324314194363\n",
      "Mean loss for tree #98 is: 16.650089311064317 The training set R Squared is: 0.8177771143543193\n",
      "Mean loss for tree #99 is: 16.613731235042824 The training set R Squared is: 0.818175026545977\n",
      "Mean loss for tree #100 is: 16.568060570642835 The training set R Squared is: 0.8186748581144923\n",
      "Mean loss for tree #101 is: 16.544685520126023 The training set R Squared is: 0.8189306807156677\n",
      "Mean loss for tree #102 is: 16.51780887058641 The training set R Squared is: 0.8192248257225834\n",
      "Mean loss for tree #103 is: 16.451810604157195 The training set R Squared is: 0.8199471278274951\n",
      "Mean loss for tree #104 is: 16.39767578819999 The training set R Squared is: 0.8205395932607651\n",
      "Mean loss for tree #105 is: 16.38059449005068 The training set R Squared is: 0.8207265354075106\n",
      "Mean loss for tree #106 is: 16.33819201258055 The training set R Squared is: 0.8211905990926329\n",
      "Mean loss for tree #107 is: 16.27717296363512 The training set R Squared is: 0.8218584073530198\n",
      "Mean loss for tree #108 is: 16.25013829634717 The training set R Squared is: 0.8221542817470627\n",
      "Mean loss for tree #109 is: 16.225473795550666 The training set R Squared is: 0.8224242164257385\n",
      "Mean loss for tree #110 is: 16.176297558924116 The training set R Squared is: 0.822962413883776\n",
      "Mean loss for tree #111 is: 16.12862871117086 The training set R Squared is: 0.8234841140879496\n",
      "Mean loss for tree #112 is: 16.095841743962765 The training set R Squared is: 0.823842943140727\n",
      "Mean loss for tree #113 is: 16.075282988193152 The training set R Squared is: 0.8240679434834615\n",
      "Mean loss for tree #114 is: 16.044945977349933 The training set R Squared is: 0.8243999595798605\n",
      "Mean loss for tree #115 is: 16.003625114756137 The training set R Squared is: 0.8248521857919024\n",
      "Mean loss for tree #116 is: 15.958388698050003 The training set R Squared is: 0.8253472648413018\n",
      "Mean loss for tree #117 is: 15.914339114570819 The training set R Squared is: 0.825829354880762\n",
      "Mean loss for tree #118 is: 15.859939713637438 The training set R Squared is: 0.8264247160004622\n",
      "Mean loss for tree #119 is: 15.8250229392768 The training set R Squared is: 0.8268068542138147\n",
      "Mean loss for tree #120 is: 15.804581760182092 The training set R Squared is: 0.8270305677670043\n",
      "Mean loss for tree #121 is: 15.770558785474776 The training set R Squared is: 0.8274029240056731\n",
      "Mean loss for tree #122 is: 15.747588279966658 The training set R Squared is: 0.8276543191615932\n",
      "Mean loss for tree #123 is: 15.716915519024196 The training set R Squared is: 0.8279900097939533\n",
      "Mean loss for tree #124 is: 15.67665997496305 The training set R Squared is: 0.8284305768843114\n",
      "Mean loss for tree #125 is: 15.628119248651815 The training set R Squared is: 0.828961819153025\n",
      "Mean loss for tree #126 is: 15.562512942907478 The training set R Squared is: 0.8296798315387792\n",
      "Mean loss for tree #127 is: 15.539227818169023 The training set R Squared is: 0.8299346699689653\n",
      "Mean loss for tree #128 is: 15.494625363573974 The training set R Squared is: 0.8304228107729714\n",
      "Mean loss for tree #129 is: 15.470605895721363 The training set R Squared is: 0.8306856860442068\n",
      "Mean loss for tree #130 is: 15.454776611839648 The training set R Squared is: 0.8308589258228531\n",
      "Mean loss for tree #131 is: 15.429510401436328 The training set R Squared is: 0.8311354457671587\n",
      "Mean loss for tree #132 is: 15.411295343024962 The training set R Squared is: 0.8313347960795767\n",
      "Mean loss for tree #133 is: 15.385012491895504 The training set R Squared is: 0.8316224424030545\n",
      "Mean loss for tree #134 is: 15.339737235362625 The training set R Squared is: 0.8321179465255586\n",
      "Mean loss for tree #135 is: 15.302066543164472 The training set R Squared is: 0.8325302243283064\n",
      "Mean loss for tree #136 is: 15.27084189966654 The training set R Squared is: 0.8328719549061417\n",
      "Mean loss for tree #137 is: 15.234146734970523 The training set R Squared is: 0.833273556283489\n",
      "Mean loss for tree #138 is: 15.214738109818407 The training set R Squared is: 0.8334859692991565\n",
      "Mean loss for tree #139 is: 15.18931970747461 The training set R Squared is: 0.8337641548714296\n",
      "Mean loss for tree #140 is: 15.149210325878423 The training set R Squared is: 0.8342031223219623\n",
      "Mean loss for tree #141 is: 15.100647356622577 The training set R Squared is: 0.8347346080232119\n",
      "Mean loss for tree #142 is: 15.07316584502604 The training set R Squared is: 0.8350353727969928\n",
      "Mean loss for tree #143 is: 15.047124485832432 The training set R Squared is: 0.8353203761702258\n",
      "Mean loss for tree #144 is: 15.007931010126278 The training set R Squared is: 0.8357493197097001\n",
      "Mean loss for tree #145 is: 14.957994827049179 The training set R Squared is: 0.8362958342183309\n",
      "Mean loss for tree #146 is: 14.92450444181377 The training set R Squared is: 0.8366623616600087\n",
      "Mean loss for tree #147 is: 14.900670513566823 The training set R Squared is: 0.8369232063378012\n",
      "Mean loss for tree #148 is: 14.84979254329225 The training set R Squared is: 0.8374800280092034\n",
      "Mean loss for tree #149 is: 14.813176932751967 The training set R Squared is: 0.837880758725277\n",
      "Loss Squared Mean: 29.40649632033283 R Squared:  0.6772911528149352\n"
     ]
    }
   ],
   "source": [
    "my_regressor.ntrees = 150\n",
    "my_regressor.max_depth=2\n",
    "\n",
    "fit_model_3 = my_regressor.fit_decisiontree(X_tr, y_tr)\n",
    "f0_3, models_3, training_predictions_3, training_means_sq_err_3, training_r_squared_3  = my_regressor.boost_gradient(X_tr, y_tr)\n",
    "y_pred_3 = my_regressor.predict(X_tst, f0_3, models_3)\n",
    "\n",
    "loss_3, r_squared_3 = my_regressor.loss_r_squared(y_tst, y_pred_3)\n",
    "\n",
    "print('Loss Squared Mean:', loss_3.mean(), 'R Squared: ', r_squared_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
